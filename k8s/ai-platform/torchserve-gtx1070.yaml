# TorchServe for GTX 1070 - Local Development Configuration
# Optimized for 8GB VRAM with efficient memory management

apiVersion: apps/v1
kind: Deployment
metadata:
  name: torchserve-gtx1070
  namespace: gameforge
  labels:
    app: torchserve
    component: gtx1070-inference
    gpu-model: gtx1070
    version: "0.8.2"
spec:
  replicas: 1  # Single replica for local GTX 1070
  selector:
    matchLabels:
      app: torchserve
      component: gtx1070-inference
  template:
    metadata:
      labels:
        app: torchserve
        component: gtx1070-inference
        gpu-model: gtx1070
        version: "0.8.2"
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8082"
        prometheus.io/path: "/metrics"
    spec:
      # Node selector for local machine with GTX 1070
      nodeSelector:
        kubernetes.io/arch: amd64
      
      # Medium priority for local development
      priorityClassName: medium-priority-inference
      
      # Service account
      serviceAccountName: torchserve-service-account
      
      # Init container for model preparation
      initContainers:
      - name: model-downloader
        image: curlimages/curl:latest
        command:
        - sh
        - -c
        - |
          echo "Preparing models for GTX 1070..."
          mkdir -p /models/gameforge-ai
          
          # Create demo model files for local testing
          echo "Demo GameForge AI model for GTX 1070" > /models/gameforge-ai/demo-model.txt
          echo "Lightweight inference model optimized for 8GB VRAM" > /models/gameforge-ai/lightweight-model.txt
          
          # Download lightweight models if available
          # For demo: create placeholder model files
          echo '{"model": "gameforge-demo", "version": "1.0", "gpu": "gtx1070", "memory": "8GB"}' > /models/gameforge-ai/model-config.json
          
          echo "Model preparation completed for GTX 1070"
        volumeMounts:
        - name: model-store
          mountPath: /models
        securityContext:
          runAsUser: 1000
          runAsGroup: 1000
      
      containers:
      - name: torchserve
        image: pytorch/torchserve:0.8.2-gpu
        imagePullPolicy: IfNotPresent
        
        # GTX 1070 optimized resource allocation
        resources:
          requests:
            cpu: "1"
            memory: "4Gi"
            # Note: nvidia.com/gpu requires GPU operator in production
            # For local demo, we'll use CPU mode first
          limits:
            cpu: "4"
            memory: "8Gi"
            # nvidia.com/gpu: "1"  # Uncomment when GPU operator is available
        
        # Environment variables optimized for GTX 1070
        env:
        - name: TS_CONFIG_FILE
          value: "/config/config-gtx1070.properties"
        - name: TS_MODEL_STORE
          value: "/models"
        - name: TS_LOAD_MODELS
          value: "gameforge-demo=demo-model.txt"
        - name: CUDA_VISIBLE_DEVICES
          value: "0"  # Use GPU 0 (GTX 1070)
        - name: NVIDIA_VISIBLE_DEVICES
          value: "0"
        - name: TS_METRICS_CONFIG
          value: "/config/metrics.yaml"
        - name: TS_ENABLE_METRICS_API
          value: "true"
        - name: TS_METRICS_MODE
          value: "prometheus"
        
        # GTX 1070 memory optimization
        - name: PYTORCH_CUDA_ALLOC_CONF
          value: "max_split_size_mb:512"  # Optimize for 8GB VRAM
        - name: TS_DECODE_INPUT_REQUEST
          value: "true"
        - name: TS_GPU_MEMORY_FRACTION
          value: "0.7"  # Use 70% of 8GB = ~5.6GB for models
        
        # Command optimized for GTX 1070
        command:
        - torchserve
        args:
        - --start
        - --model-store=/models
        - --ts-config=/config/config-gtx1070.properties
        - --enable-model-api
        - --enable-metrics-api
        - --ncs  # No config snapshot for demo
        
        # Health checks
        livenessProbe:
          httpGet:
            path: /ping
            port: 8080
          initialDelaySeconds: 60
          periodSeconds: 30
          timeoutSeconds: 10
          failureThreshold: 3
        
        readinessProbe:
          httpGet:
            path: /ping
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        
        # Startup probe for model loading on GTX 1070
        startupProbe:
          httpGet:
            path: /ping
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 10  # Allow time for GPU initialization
        
        # Ports
        ports:
        - containerPort: 8080
          name: inference
          protocol: TCP
        - containerPort: 8081
          name: management
          protocol: TCP
        - containerPort: 8082
          name: metrics
          protocol: TCP
        
        # Volume mounts
        volumeMounts:
        - name: model-store
          mountPath: /models
        - name: config
          mountPath: /config
        - name: logs
          mountPath: /logs
        - name: tmp
          mountPath: /tmp
        
        # Security context
        securityContext:
          runAsUser: 1000
          runAsGroup: 1000
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: false
          capabilities:
            drop:
            - ALL
      
      # Volumes
      volumes:
      - name: model-store
        emptyDir:
          sizeLimit: 10Gi  # Sufficient for GTX 1070 models
      - name: config
        configMap:
          name: torchserve-gtx1070-config
      - name: logs
        emptyDir: {}
      - name: tmp
        emptyDir:
          sizeLimit: 2Gi
      
      # Security context
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000

---
# TorchServe Service for GTX 1070
apiVersion: v1
kind: Service
metadata:
  name: torchserve-gtx1070
  namespace: gameforge
  labels:
    app: torchserve
    component: gtx1070-inference
    gpu-model: gtx1070
spec:
  type: ClusterIP
  selector:
    app: torchserve
    component: gtx1070-inference
  ports:
  - name: inference
    port: 8080
    targetPort: 8080
    protocol: TCP
  - name: management
    port: 8081
    targetPort: 8081
    protocol: TCP
  - name: metrics
    port: 8082
    targetPort: 8082
    protocol: TCP

---
# NodePort Service for External Access
apiVersion: v1
kind: Service
metadata:
  name: torchserve-gtx1070-external
  namespace: gameforge
  labels:
    app: torchserve
    component: external-access
spec:
  type: NodePort
  selector:
    app: torchserve
    component: gtx1070-inference
  ports:
  - name: inference
    port: 8080
    targetPort: 8080
    nodePort: 30080  # Access via localhost:30080
    protocol: TCP
  - name: management
    port: 8081
    targetPort: 8081
    nodePort: 30081  # Management via localhost:30081
    protocol: TCP

---
# TorchServe GTX 1070 Configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: torchserve-gtx1070-config
  namespace: gameforge
data:
  config-gtx1070.properties: |
    # TorchServe GTX 1070 Configuration
    # Optimized for 8GB VRAM and local development
    
    inference_address=http://0.0.0.0:8080
    management_address=http://0.0.0.0:8081
    metrics_address=http://0.0.0.0:8082
    
    # GTX 1070 optimized settings
    number_of_netty_threads=4
    netty_client_threads=4
    default_workers_per_model=1  # Conservative for 8GB VRAM
    max_workers=2               # Max 2 workers to avoid OOM
    batch_size=4                # Small batch for memory efficiency
    max_batch_delay=100
    response_timeout=120
    unregister_model_timeout=120
    decode_input_request=true
    
    # GPU memory management for GTX 1070
    gpu_memory_fraction=0.7     # Use 70% of 8GB
    
    # Model management
    model_store=/models
    
    # Logging optimized for development
    default_response_timeout=120
    vmargs=-Dlog4j2.formatMsgNoLookups=true -Xmx4g -XX:MaxDirectMemorySize=1g -XX:+UseG1GC
    
    # CORS settings for web integration
    cors_allowed_origin=*
    cors_allowed_methods=GET,POST,PUT,DELETE
    cors_allowed_headers=*
    
    # Metrics
    metrics_config=/config/metrics.yaml
    enable_metrics_api=true
    metrics_mode=prometheus
    
    # Development settings
    install_py_dep_per_model=false
    model_server_home=/tmp
    
    # GTX 1070 specific optimizations
    prefer_direct_buffer=true
    max_request_size=65535000   # ~65MB max request
    max_response_size=65535000  # ~65MB max response
  
  metrics.yaml: |
    # TorchServe Metrics Configuration for GTX 1070
    metrics:
      counter:
        - name: "Requests2XX"
          unit: "Count"
          dimensions: ["Level", "Hostname"]
        - name: "Requests4XX" 
          unit: "Count"
          dimensions: ["Level", "Hostname"]
        - name: "Requests5XX"
          unit: "Count"
          dimensions: ["Level", "Hostname"]
      
      gauge:
        - name: "CPUUtilization"
          unit: "Percent"
          dimensions: ["Level", "Hostname"]
        - name: "MemoryUtilization"
          unit: "Percent"
          dimensions: ["Level", "Hostname"]
        - name: "GPUUtilization"
          unit: "Percent"
          dimensions: ["Level", "Hostname", "DeviceId"]
        - name: "GPUMemoryUtilization"
          unit: "Percent"
          dimensions: ["Level", "Hostname", "DeviceId"]
      
      histogram:
        - name: "InferenceLatency"
          unit: "Milliseconds"
          dimensions: ["Level", "Hostname", "ModelName", "ModelVersion"]

---
# Service Account for TorchServe
apiVersion: v1
kind: ServiceAccount
metadata:
  name: torchserve-service-account
  namespace: gameforge

---
# Priority Class for GTX 1070 Workloads
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: medium-priority-inference
value: 1000
globalDefault: false
description: "Medium priority for GTX 1070 inference workloads"

---
# ConfigMap for Demo Models
apiVersion: v1
kind: ConfigMap
metadata:
  name: demo-models
  namespace: gameforge
data:
  create-demo-model.py: |
    #!/usr/bin/env python3
    """
    Create demo PyTorch model for GTX 1070 testing
    """
    import torch
    import torch.nn as nn
    import torchvision.transforms as transforms
    from torch.jit import script
    import os
    
    class GameForgeDemo(nn.Module):
        """Simple demo model for GTX 1070"""
        def __init__(self):
            super(GameForgeDemo, self).__init__()
            self.conv1 = nn.Conv2d(3, 32, 3, padding=1)
            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)
            self.pool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(64, 10)
            self.relu = nn.ReLU()
        
        def forward(self, x):
            x = self.relu(self.conv1(x))
            x = self.relu(self.conv2(x))
            x = self.pool(x)
            x = torch.flatten(x, 1)
            x = self.fc(x)
            return x
    
    def create_model():
        """Create and save demo model"""
        model = GameForgeDemo()
        model.eval()
        
        # Create example input (batch_size=1, channels=3, height=224, width=224)
        example_input = torch.randn(1, 3, 224, 224)
        
        # Script the model for TorchServe
        scripted_model = script(model)
        
        # Save the model
        os.makedirs('/models/gameforge-ai', exist_ok=True)
        scripted_model.save('/models/gameforge-ai/gameforge-demo.pt')
        
        print("Demo model created successfully for GTX 1070!")
        print(f"Model size: {os.path.getsize('/models/gameforge-ai/gameforge-demo.pt')} bytes")
        
        # Test inference
        with torch.no_grad():
            output = scripted_model(example_input)
            print(f"Test inference output shape: {output.shape}")
    
    if __name__ == "__main__":
        create_model()

---
# Job to Create Demo Models
apiVersion: batch/v1
kind: Job
metadata:
  name: create-demo-models
  namespace: gameforge
spec:
  template:
    spec:
      containers:
      - name: model-creator
        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
        command:
        - python3
        - /scripts/create-demo-model.py
        volumeMounts:
        - name: scripts
          mountPath: /scripts
        - name: model-store
          mountPath: /models
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "2"
            memory: "4Gi"
      volumes:
      - name: scripts
        configMap:
          name: demo-models
          defaultMode: 0755
      - name: model-store
        emptyDir: {}
      restartPolicy: OnFailure
  backoffLimit: 3
