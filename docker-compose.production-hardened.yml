# GameForge Production Stack - Maximum Security Hardening
# ========================================================================
# Enterprise production deployment with comprehensive security hardening:
# - Seccomp profiles for syscall filtering
# - AppArmor profiles for mandatory access control
# - Dropped capabilities and security contexts
# - Network isolation and resource limits
# - Read-only filesystems and tmpfs mounts
# ========================================================================

version: '3.8'

# ========================================================================
# Security Context Templates
# ========================================================================
x-app-security: &app-security
  user: "1001:1001"
  read_only: false  # Phase 4: Allow model cache and temporary files
  tmpfs:
    - /tmp:size=2G,mode=1777  # Phase 4: Increased for model storage
    - /app/tmp:noexec,nosuid,size=500m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/gameforge-app.json
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - FOWNER
    - DAC_OVERRIDE

# Phase 4: Vault Security Template
x-vault-security: &vault-security
  user: "100:1000"
  read_only: true
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/vault.json
  cap_drop:
    - ALL
  cap_add:
    - IPC_LOCK
    - SETUID
    - SETGID

x-web-security: &web-security
  user: "101:101"
  read_only: true
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
    - /var/cache/nginx:noexec,nosuid,size=500m
    - /var/run:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/nginx.json
    - apparmor:nginx-container
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - NET_BIND_SERVICE
    - DAC_OVERRIDE

x-db-security: &db-security
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
    - /var/run:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/database.json
    - apparmor:database-container
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - FOWNER
    - DAC_OVERRIDE

# ========================================================================
# Resource Limit Templates
# ========================================================================
x-app-resources: &app-resources
  deploy:
    resources:
      limits:
        memory: 8G
        cpus: '4.0'
        pids: 1000
      reservations:
        memory: 2G
        cpus: '1.0'
    restart_policy:
      condition: on-failure
      delay: 30s
      max_attempts: 3

# Phase 4: Vault Resource Template
x-vault-resources: &vault-resources
  deploy:
    resources:
      limits:
        memory: 1G
        cpus: '1.0'
        pids: 100
      reservations:
        memory: 256M
        cpus: '0.25'
    restart_policy:
      condition: on-failure
      delay: 15s
      max_attempts: 5

x-db-resources: &db-resources
  deploy:
    resources:
      limits:
        memory: 4G
        cpus: '2.0'
        pids: 500
      reservations:
        memory: 1G
        cpus: '0.5'

x-web-resources: &web-resources
  deploy:
    resources:
      limits:
        memory: 1G
        cpus: '1.0'
        pids: 200
      reservations:
        memory: 256M
        cpus: '0.25'

# ========================================================================
# Common Configuration
# ========================================================================
x-common-env: &common-env
  GAMEFORGE_ENV: production
  LOG_LEVEL: info
  TZ: UTC
  PYTHONPATH: /app

x-secure-logging: &secure-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"
    labels: "service,environment,security"

services:
  # ========================================================================
  # Security Bootstrap Service (One-shot privileged initialization)
  # ========================================================================
  security-bootstrap:
    image: gameforge-security-init:latest
    container_name: gameforge-security-bootstrap
    restart: "no"  # One-shot execution only
    privileged: true
    volumes:
      - security-shared:/shared
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
    environment:
      - SECURITY_MODE=bootstrap
      - LOG_LEVEL=info
      - SECURITY_GATE_THRESHOLD=75
      - BOOTSTRAP_ONLY=true
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
        labels: "service=security-bootstrap,environment=production,security=critical"
    labels:
      - "traefik.enable=false"
      - "security.role=bootstrap"
      - "security.criticality=high"
      - "security.execution=one-shot"
    # No healthcheck for one-shot bootstrap
    entrypoint: ["/usr/local/bin/security-bootstrap.sh"]

  # ========================================================================
  # Security Health Monitor (Non-privileged periodic monitoring)
  # ========================================================================
  security-monitor:
    image: gameforge-security-init:latest
    container_name: gameforge-security-monitor
    restart: unless-stopped
    privileged: false  # No privileges needed for monitoring
    user: "1001:1001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/monitor:nosuid,size=50m  # Writable space for monitor data (removed noexec)
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json  # Fixed with arch_prctl and rseq
    cap_drop:
      - ALL
    volumes:
      - security-shared:/shared:ro  # Read-only access to security data
    environment:
      - SECURITY_MODE=monitor
      - LOG_LEVEL=info
      - MONITOR_INTERVAL=300  # 5 minutes
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
        labels: "service=security-monitor,environment=production,security=monitoring"
    labels:
      - "traefik.enable=false"
      - "security.role=monitoring"
      - "security.criticality=medium"
    healthcheck:
      test: ["CMD", "/usr/local/bin/monitor-health-check.sh"]
      interval: 60s
      timeout: 10s
      start_period: 30s
      retries: 3
    entrypoint: ["/usr/local/bin/security-monitor.sh", "loop"]
    depends_on:
      security-bootstrap:
        condition: service_completed_successfully

  # ========================================================================
  # GameForge Application Service (Phase 2: Enhanced Multi-stage Build + Phase 4: Model Asset Security)
  # ========================================================================
  gameforge-app:
    build:
      context: .
      dockerfile: Dockerfile.production
      target: production
      args:
        # Phase 2: Enhanced Multi-stage Build Arguments
        BUILD_DATE: ${BUILD_DATE:-}
        VCS_REF: ${VCS_REF:-}
        BUILD_VERSION: ${BUILD_VERSION:-latest}
        VARIANT: ${GAMEFORGE_VARIANT:-gpu}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.10}
        DEBIAN_FRONTEND: noninteractive
        # Phase 2: Dynamic base image selection
        GPU_BASE_IMAGE: ${GPU_BASE_IMAGE:-nvidia/cuda:12.1-devel-ubuntu22.04}
        CPU_BASE_IMAGE: ${CPU_BASE_IMAGE:-ubuntu:22.04}
        # Phase 2: Build optimization flags
        BUILD_ENV: phase2-phase4-production
        ENABLE_GPU: ${ENABLE_GPU:-true}
        COMPILE_BYTECODE: ${COMPILE_BYTECODE:-true}
        SECURITY_HARDENING: ${SECURITY_HARDENING:-true}
    image: gameforge:phase2-phase4-production-${GAMEFORGE_VARIANT:-gpu}
    container_name: gameforge-app-phase2-phase4-secure
    hostname: gameforge-app
    restart: unless-stopped
    <<: *app-security
    # Phase 2: Dynamic GPU/CPU runtime configuration
    # runtime: ${DOCKER_RUNTIME:-nvidia}  # Commented out for CPU deployment
    deploy:
      resources:
        limits:
          # Phase 2: Dynamic resource limits based on variant
          memory: ${MEMORY_LIMIT:-12G}  # Configurable memory limit
          cpus: ${CPU_LIMIT:-6.0}      # Configurable CPU limit
          pids: 1000
        reservations:
          memory: ${MEMORY_RESERVATION:-4G}
          cpus: ${CPU_RESERVATION:-2.0}
          # Phase 2: GPU reservations for GPU variant (disabled for CPU deployment)
          # devices:
          #   - driver: nvidia
          #     count: ${GPU_COUNT:-1}
          #     capabilities: [gpu]
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
    environment:
      <<: *common-env
      # Phase 2: Variant-specific environment
      GAMEFORGE_VARIANT: ${GAMEFORGE_VARIANT:-gpu}
      BUILD_VERSION: ${BUILD_VERSION:-latest}
      VCS_REF: ${VCS_REF:-}
      # Database connections
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/0
      ELASTICSEARCH_URL: http://elasticsearch:9200
      # Security keys
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      SECRET_KEY: ${SECRET_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      # Phase 4: Model Security Configuration
      MODEL_SECURITY_ENABLED: "true"
      SECURITY_SCAN_ENABLED: "true"
      STRICT_MODEL_SECURITY: "true"
      VAULT_HEALTH_CHECK_ENABLED: "true"
      PERFORMANCE_MONITORING_ENABLED: "true"
      # Phase 4: Vault Integration
      VAULT_ADDR: "http://vault:8200"
      VAULT_TOKEN: ${VAULT_TOKEN}
      VAULT_NAMESPACE: "gameforge"
      # Phase 4: Model Storage Configuration
      MODEL_STORAGE_BACKEND: "s3"
      AWS_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_S3_BUCKET: ${MODEL_S3_BUCKET}
      MODEL_CACHE_DIR: "/tmp/models"
      REQUIRED_MODELS: ${REQUIRED_MODELS:-""}
      # Phase 2: GPU optimization environment (when GPU variant)
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVIDIA_DRIVER_CAPABILITIES: ${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      PYTORCH_CUDA_ALLOC_CONF: ${PYTORCH_CUDA_ALLOC_CONF:-max_split_size_mb:512,garbage_collection_threshold:0.6,expandable_segments:True}
      CUDA_LAUNCH_BLOCKING: ${CUDA_LAUNCH_BLOCKING:-0}
      CUDA_CACHE_DISABLE: ${CUDA_CACHE_DISABLE:-0}
      PYTORCH_JIT: ${PYTORCH_JIT:-1}
      PYTORCH_JIT_LOG_LEVEL: ${PYTORCH_JIT_LOG_LEVEL:-ERROR}
      # Phase 2: Performance tuning
      WORKERS: ${WORKERS:-4}
      MAX_WORKERS: ${MAX_WORKERS:-8}
      WORKER_TIMEOUT: ${WORKER_TIMEOUT:-300}
      WORKER_CLASS: ${WORKER_CLASS:-uvicorn.workers.UvicornWorker}
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      # Phase 4: Enhanced script mounting and model cache
      - ./scripts:/app/scripts:ro
      - model-cache:/tmp/models:rw
      - monitoring-data:/tmp/monitoring:rw
    networks:
      - frontend
      - backend
      - monitoring
      - vault-network  # Phase 4: Vault network access
    ports:
      - "127.0.0.1:8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=phase2-phase4-production"
      - "security.compliance=required"
      - "gameforge.phase=2+4"
      - "gameforge.variant=${GAMEFORGE_VARIANT:-gpu}"
      - "phase2.multi-stage=enabled"
      - "phase2.cpu-gpu-variants=enabled"
      - "phase2.build-optimization=enabled"
      - "phase4.model-security=enabled"
      - "phase4.vault-integration=enabled"
      - "ai.gpu.enabled=${ENABLE_GPU:-true}"
      - "ai.gpu.driver=${DOCKER_RUNTIME:-nvidia}"
    depends_on:
      security-bootstrap:
        condition: service_completed_successfully
      security-monitor:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      vault:
        condition: service_healthy
    # Phase 4: Enhanced entrypoint
    entrypoint: ["/app/scripts/entrypoint-phase4.sh"]
    command: ["python", "-m", "gameforge.main"]

  # ========================================================================
  # Nginx Reverse Proxy (Maximum Security)
  # ========================================================================
  nginx:
    image: nginx:1.24.0-alpine
    container_name: gameforge-nginx-secure
    hostname: nginx
    restart: unless-stopped
    <<: [*web-security, *web-resources]
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl/certs:/etc/ssl/certs:ro
      - ./ssl/private:/etc/ssl/private:ro
      - nginx-logs:/var/log/nginx:rw
      - static-files:/var/www/html:ro
    networks:
      - frontend
    ports:
      - "80:80"
      - "443:443"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=web-server"
    depends_on:
      security-monitor:
        condition: service_healthy
      gameforge-app:
        condition: service_healthy

  # ========================================================================
  # PostgreSQL Database (Hardened)
  # ========================================================================
  postgres:
    image: postgres:15.4-alpine
    container_name: gameforge-postgres-secure
    hostname: postgres
    restart: unless-stopped
    <<: [*db-security, *db-resources]
    user: "999:999"
    environment:
      POSTGRES_DB: gameforge_prod
      POSTGRES_USER: gameforge
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    volumes:
      - postgres-data:/var/lib/postgresql/data:rw
      - ./database_setup.sql:/docker-entrypoint-initdb.d/01-setup.sql:ro
      - postgres-logs:/var/log/postgresql:rw
    tmpfs:
      - /var/lib/postgresql/data/pg_stat_tmp:noexec,nosuid,size=100m
    networks:
      - backend
    ports:
      - "127.0.0.1:5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U gameforge -d gameforge_prod"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=database"
    command: >
      postgres
      -c ssl=on
      -c ssl_cert_file=/var/lib/postgresql/server.crt
      -c ssl_key_file=/var/lib/postgresql/server.key
      -c shared_preload_libraries=pg_stat_statements
      -c max_connections=200
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
      -c log_statement=mod
      -c log_min_duration_statement=1000

  # ========================================================================
  # Redis Cache (Hardened)
  # ========================================================================
  redis:
    image: redis:7.2.1-alpine
    container_name: gameforge-redis-secure
    hostname: redis
    restart: unless-stopped
    <<: *db-security
    user: "999:999"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data:rw
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - backend
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=cache"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru

  # ========================================================================
  # HashiCorp Vault (Phase 4: Model Asset Security)
  # ========================================================================
  vault:
    image: hashicorp/vault:latest
    container_name: gameforge-vault-secure
    hostname: vault
    restart: unless-stopped
    <<: [*vault-security, *vault-resources]
    environment:
      VAULT_ADDR: "http://0.0.0.0:8200"
      VAULT_API_ADDR: "http://vault:8200"
      VAULT_CLUSTER_ADDR: "http://vault:8201"
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_ROOT_TOKEN}
      VAULT_DEV_LISTEN_ADDRESS: "0.0.0.0:8200"
      VAULT_LOCAL_CONFIG: |
        {
          "backend": {
            "file": {
              "path": "/vault/data"
            }
          },
          "listener": {
            "tcp": {
              "address": "0.0.0.0:8200",
              "tls_disable": true
            }
          },
          "default_lease_ttl": "168h",
          "max_lease_ttl": "720h",
          "ui": true,
          "log_level": "INFO"
        }
    volumes:
      - vault-data:/vault/data:rw
      - vault-logs:/vault/logs:rw
      - ./vault/config:/vault/config:ro
      - ./vault/policies:/vault/policies:ro
    networks:
      - backend
      - vault-network
    ports:
      - "127.0.0.1:8200:8200"
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=vault"
      - "phase4.vault=enabled"
      - "phase4.model-security=enabled"
    command: >
      vault server
      -config=/vault/config
      -dev-root-token-id=${VAULT_ROOT_TOKEN}
      -dev-listen-address=0.0.0.0:8200

  # ========================================================================
  # Elasticsearch (Hardened)
  # ========================================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.2
    container_name: gameforge-elasticsearch-secure
    hostname: elasticsearch
    restart: unless-stopped
    user: "1000:1000"
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    environment:
      - node.name=gameforge-elasticsearch
      - cluster.name=gameforge-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - xpack.license.self_generated.type=basic
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data:rw
      - elasticsearch-logs:/usr/share/elasticsearch/logs:rw
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    networks:
      - backend
      - monitoring
    ports:
      - "127.0.0.1:9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -u elastic:${ELASTIC_PASSWORD} -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=search-engine"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '0.5'

  # ========================================================================
  # Logstash Log Processing (Hardened)
  # ========================================================================
  logstash:
    image: docker.elastic.co/logstash/logstash:8.9.2
    container_name: gameforge-logstash-secure
    hostname: logstash
    restart: unless-stopped
    user: "1000:1000"
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    environment:
      - "LS_JAVA_OPTS=-Xmx1g -Xms1g"
      - LOGSTASH_SYSTEM_PASSWORD=${LOGSTASH_SYSTEM_PASSWORD}
      - xpack.monitoring.enabled=true
      - xpack.monitoring.elasticsearch.hosts=["elasticsearch:9200"]
      - xpack.monitoring.elasticsearch.username=logstash_system
      - xpack.monitoring.elasticsearch.password=${LOGSTASH_SYSTEM_PASSWORD}
    volumes:
      - ./monitoring/logging/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - logstash-data:/usr/share/logstash/data:rw
    networks:
      - backend
      - monitoring
    ports:
      - "127.0.0.1:5044:5044"  # Beats input
      - "127.0.0.1:8080:8080"  # HTTP input
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9600/_node/stats"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=log-processor"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.2'
    depends_on:
      elasticsearch:
        condition: service_healthy

  # ========================================================================
  # Filebeat Log Collection (Hardened)
  # ========================================================================
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.9.2
    container_name: gameforge-filebeat-secure
    hostname: filebeat
    restart: unless-stopped
    user: "root"  # Required for log file access
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /usr/share/filebeat/data:noexec,nosuid,size=500m
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
    command: [
      "--strict.perms=false",
      "-e",
      "-E", "output.logstash.hosts=[\"logstash:5044\"]",
      "-E", "setup.kibana.host=grafana:3000"
    ]
    environment:
      - FILEBEAT_SYSTEM_PASSWORD=${FILEBEAT_SYSTEM_PASSWORD}
      - NODE_NAME=gameforge-filebeat
    volumes:
      - ./monitoring/logging/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - gameforge-logs:/var/log/gameforge:ro
      - nginx-logs:/var/log/nginx:ro
      - postgres-logs:/var/log/postgresql:ro
      - elasticsearch-logs:/var/log/elasticsearch:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - backend
      - monitoring
    healthcheck:
      test: ["CMD", "filebeat", "test", "config"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=log-collector"
      - "co.elastic.logs/enabled=false"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    depends_on:
      logstash:
        condition: service_healthy

  # ========================================================================
  # Background Workers (Hardened)
  # ========================================================================
  gameforge-worker:
    build:
      context: .
      dockerfile: Dockerfile.production
      target: production
    image: gameforge:production-secure
    container_name: gameforge-worker-secure
    hostname: gameforge-worker
    restart: unless-stopped
    <<: *app-security
    # GPU optimization for AI worker tasks
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
          pids: 500
        reservations:
          memory: 1G
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
    environment:
      <<: *common-env
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/0
      ELASTICSEARCH_URL: http://elasticsearch:9200
      WORKER_TYPE: background
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/2
      # GPU optimization environment for workers
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:256,garbage_collection_threshold:0.6,expandable_segments:True
      CUDA_LAUNCH_BLOCKING: 0
      CUDA_CACHE_DISABLE: 0
      PYTORCH_JIT: 1
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      - gameforge-models:/app/models_cache:rw
    networks:
      - backend
      - monitoring
    healthcheck:
      test: ["CMD", "python", "-c", "import celery; print('Worker healthy')"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=worker"
      - "ai.gpu.enabled=true"
      - "ai.gpu.driver=nvidia"
    command: ["celery", "worker", "-A", "gameforge.celery", "--loglevel=info", "--concurrency=4"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  # ========================================================================
  # Backup Service (Hardened)
  # ========================================================================
  backup-service:
    image: postgres:15.4-alpine
    container_name: gameforge-backup-secure
    hostname: backup-service
    restart: unless-stopped
    user: "1001:1001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
      - /var/run:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    environment:
      POSTGRES_DB: gameforge_prod
      POSTGRES_USER: gameforge
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      BACKUP_SCHEDULE: "0 2 * * *"  # Daily at 2 AM
      BACKUP_RETENTION_DAYS: 30
      S3_BUCKET: ${BACKUP_S3_BUCKET}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - backup-data:/backups:rw
      - ./scripts/backup.sh:/usr/local/bin/backup.sh:ro
      - ./scripts/restore.sh:/usr/local/bin/restore.sh:ro
    networks:
      - backend
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=backup"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: ["sh", "-c", "crond -f -d 8"]
    depends_on:
      postgres:
        condition: service_healthy

  # ========================================================================
  # Prometheus Monitoring (Hardened)
  # ========================================================================
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: gameforge-prometheus-secure
    hostname: prometheus
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus:rw
    networks:
      - monitoring
    ports:
      - "127.0.0.1:9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=monitoring"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.2'

  # ========================================================================
  # Grafana Dashboard (Hardened)
  # ========================================================================
  grafana:
    image: grafana/grafana:10.1.2
    container_name: gameforge-grafana-secure
    hostname: grafana
    restart: unless-stopped
    user: "472:472"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/lib/grafana/plugins:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SECURITY_DISABLE_GRAVATAR: true
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
      GF_SECURITY_COOKIE_SECURE: true
      GF_SECURITY_COOKIE_SAMESITE: strict
      GF_SERVER_ROOT_URL: https://${DOMAIN}/grafana/
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana:rw
    networks:
      - monitoring
      - frontend
    ports:
      - "127.0.0.1:3000:3000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=dashboard"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    depends_on:
      prometheus:
        condition: service_healthy

  # ========================================================================
  # Phase 3: Image Security Pipeline Services
  # ========================================================================

  # Security Scanner Service (Trivy + Grype)
  security-scanner:
    image: aquasec/trivy:latest
    container_name: gameforge-security-scanner
    command: ["server", "--listen", "0.0.0.0:8080"]
    environment:
      TRIVY_LISTEN: "0.0.0.0:8080"
      TRIVY_CACHE_DIR: "/cache"
      TRIVY_DB_REPOSITORY: "ghcr.io/aquasecurity/trivy-db"
      TRIVY_JAVA_DB_REPOSITORY: "ghcr.io/aquasecurity/trivy-java-db"
      TRIVY_LOG_LEVEL: "INFO"
      # Phase 3: Security scanning configuration
      TRIVY_SEVERITY: "UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL"
      TRIVY_SECURITY_CHECKS: "vuln,secret,config"
      TRIVY_TIMEOUT: "10m"
    volumes:
      - security-scanner-cache:/cache:rw
      - security-scanner-reports:/reports:rw
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./security/scripts:/scripts:ro
    networks:
      - security-pipeline
      - monitoring
    ports:
      - "127.0.0.1:8082:8080"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=scanner"
      - "security.component=phase3-pipeline"

  # SBOM Generator Service
  sbom-generator:
    image: anchore/syft:latest
    container_name: gameforge-sbom-generator
    command: ["serve", "--host", "0.0.0.0", "--port", "8080"]
    environment:
      SYFT_LOG_LEVEL: "info"
      SYFT_QUIET: "false"
      # Phase 3: SBOM generation configuration
      SYFT_OUTPUT_FORMAT: "spdx-json"
      SYFT_CATALOGERS: "all"
      SYFT_SCOPE: "all-layers"
    volumes:
      - sbom-reports:/reports:rw
      - sbom-cache:/cache:rw
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./security/scripts:/scripts:ro
    networks:
      - security-pipeline
      - monitoring
    ports:
      - "127.0.0.1:8083:8080"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=sbom"
      - "security.component=phase3-pipeline"

  # Image Signing Service (Cosign)
  image-signer:
    image: gcr.io/projectsigstore/cosign:latest
    container_name: gameforge-image-signer
    command: ["sleep", "infinity"]
    environment:
      COSIGN_EXPERIMENTAL: "1"
      COSIGN_PRIVATE_KEY: "/keys/cosign.key"
      COSIGN_PUBLIC_KEY: "/keys/cosign.pub"
      # Phase 3: Image signing configuration
      SIGSTORE_ROOT_FILE: "/etc/ssl/certs/fulcio-root.crt"
      REKOR_URL: "https://rekor.sigstore.dev"
    volumes:
      - cosign-keys:/keys:ro
      - signing-reports:/reports:rw
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./security/scripts:/scripts:ro
    networks:
      - security-pipeline
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=200m
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.05'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=signer"
      - "security.component=phase3-pipeline"

  # Harbor Registry (Enterprise Container Registry)
  harbor-registry:
    image: goharbor/harbor-core:latest
    container_name: gameforge-harbor-registry
    environment:
      CORE_SECRET: ${HARBOR_CORE_SECRET}
      JOBSERVICE_SECRET: ${HARBOR_JOBSERVICE_SECRET}
      # Phase 3: Harbor configuration
      HARBOR_ADMIN_PASSWORD: ${HARBOR_ADMIN_PASSWORD}
      DATABASE_TYPE: "postgresql"
      DATABASE_HOSTNAME: "postgres"
      DATABASE_USERNAME: ${POSTGRES_USER}
      DATABASE_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASE_DATABASE: "harbor_registry"
      DATABASE_SSL_MODE: "require"
      REDIS_URL: "redis://redis:6379/2"
      CLAIR_DB_HOST: "postgres"
      CLAIR_DB_PASSWORD: ${POSTGRES_PASSWORD}
      CLAIR_DB_USERNAME: ${POSTGRES_USER}
      CLAIR_DB: "harbor_clair"
      CLAIR_URL: "http://clair-scanner:6060"
      NOTARY_URL: "http://notary-server:4443"
      REGISTRY_URL: "http://registry:5000"
      TOKEN_SERVICE_URL: "http://harbor-registry:8080/service/token"
      WITH_NOTARY: "true"
      WITH_CLAIR: "true"
      WITH_CHARTMUSEUM: "false"
      LOG_LEVEL: "info"
      CONFIG_PATH: "/etc/harbor"
      SHARED_PATH: "/shared"
      REGISTRY_CONTROLLER_URL: "http://registryctl:8080"
      PORTAL_URL: "http://harbor-portal:8080"
      REGISTRYCTL_URL: "http://registryctl:8080"
    volumes:
      - harbor-registry-data:/data:rw
      - harbor-registry-logs:/var/log/harbor:rw
      - ./security/configs/harbor.yml:/etc/harbor/harbor.yml:ro
      - harbor-registry-secret:/etc/harbor/secretkey:rw
      - harbor-registry-ca:/etc/harbor/ca:ro
    networks:
      - security-pipeline
      - backend
      - monitoring
    ports:
      - "127.0.0.1:8084:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v2.0/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=registry"
      - "security.component=phase3-pipeline"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  # Security Dashboard (Grafana with Security Templates)
  security-dashboard:
    image: grafana/grafana:latest
    container_name: gameforge-security-dashboard
    hostname: security-dashboard
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD:-admin}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY:-}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_USERS_ALLOW_ORG_CREATE: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      # Phase 3: Security dashboard configuration
      GF_INSTALL_PLUGINS: "grafana-clock-panel,grafana-simple-json-datasource"
      GF_RENDERING_SERVER_URL: "http://localhost:8081/render"
      GF_LOG_LEVEL: "info"
      GF_METRICS_ENABLED: "true"
      GF_SECURITY_DISABLE_GRAVATAR: "true"
      GF_SECURITY_COOKIE_SECURE: "true"
      GF_SECURITY_STRICT_TRANSPORT_SECURITY: "true"
      GF_SECURITY_CONTENT_TYPE_PROTECTION: "true"
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_ANALYTICS_CHECK_FOR_UPDATES: "false"
      GF_SNAPSHOTS_EXTERNAL_ENABLED: "false"
    volumes:
      - security-dashboard-data:/var/lib/grafana:rw
      - ./security/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./security/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - security-pipeline
      - monitoring
    ports:
      - "127.0.0.1:3001:3000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
    read_only: false  # Grafana needs write access for dashboard storage
    tmpfs:
      - /tmp:noexec,nosuid,size=200m
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=dashboard"
      - "security.component=phase3-pipeline"

# ========================================================================
# Network Configuration (Isolated)
# ========================================================================
networks:
  frontend:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "false"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1

  backend:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1

  monitoring:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1

  # Phase 4: Vault Network for Model Security
  vault-network:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.23.0.0/24
          gateway: 172.23.0.1

  # Phase 3: Security Pipeline Network
  security-pipeline:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.24.0.0/24
          gateway: 172.24.0.1

# ========================================================================
# Volume Configuration (Secure)
# ========================================================================
volumes:
  # Security infrastructure shared volume
  security-shared:
    driver: local
    
  gameforge-logs:
    driver: local

  gameforge-cache:
    driver: local

  gameforge-assets:
    driver: local

  gameforge-models:
    driver: local

  # Phase 4: Model Cache Volume (Temporary, secure)
  model-cache:
    driver: local

  # Phase 4: Monitoring Data Volume
  monitoring-data:
    driver: local

  # Phase 4: Vault Data Volume
  vault-data:
    driver: local

  # Phase 4: Vault Logs Volume
  vault-logs:
    driver: local

  postgres-data:
    driver: local

  postgres-logs:
    driver: local

  redis-data:
    driver: local

  elasticsearch-data:
    driver: local

  elasticsearch-logs:
    driver: local

  logstash-data:
    driver: local

  nginx-logs:
    driver: local

  static-files:
    driver: local

  backup-data:
    driver: local

  prometheus-data:
    driver: local

  grafana-data:
    driver: local

  # ========================================================================
  # Phase 3: Security Pipeline Volumes (Simplified for Windows deployment)
  # ========================================================================
  security-scanner-cache:
    driver: local
  security-scanner-reports:
    driver: local
  sbom-reports:
    driver: local
  sbom-cache:
    driver: local
  cosign-keys:
    driver: local
  signing-reports:
    driver: local
  policy-engine-data:
    driver: local
  policy-engine-logs:
    driver: local
  security-metrics-data:
    driver: local
  security-metrics-rules:
    driver: local
  security-dashboard-data:
    driver: local
  harbor-registry-data:
    driver: local
  harbor-registry-logs:
    driver: local
  harbor-registry-secret:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/harbor-secret
      o: bind,nodev,nosuid

  harbor-registry-ca:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/harbor-ca
      o: bind,nodev,nosuid,ro

  # Clair Scanner Volumes (Phase 3)
  clair-scanner-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/clair-data
      o: bind,nodev,nosuid

  clair-scanner-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/clair-logs
      o: bind,nodev,nosuid

  # Notary Server Volumes (Phase 3)
  notary-server-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/notary-data
      o: bind,nodev,nosuid

  notary-server-certs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/notary-certs
      o: bind,nodev,nosuid,ro
