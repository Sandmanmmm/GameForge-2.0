# TorchServe Configuration for RTX 4090 on Vast.ai
# Optimized for 24GB VRAM and high-performance inference

# =============================================================================
# Network Configuration
# =============================================================================
inference_address=http://0.0.0.0:8080
management_address=http://0.0.0.0:8081
metrics_address=http://0.0.0.0:8082

# =============================================================================
# RTX 4090 Performance Optimization
# =============================================================================
# Thread configuration for RTX 4090
number_of_netty_threads=16
netty_client_threads=16
job_queue_size=1000

# Worker configuration optimized for 24GB VRAM
default_workers_per_model=4
max_workers=16
batch_size=16           # Larger batches for RTX 4090
max_batch_delay=50      # Lower latency for cloud serving
response_timeout=300
unregister_model_timeout=300
decode_input_request=true

# =============================================================================
# GPU Memory Management for RTX 4090
# =============================================================================
# Use 70% of 24GB = ~16.8GB for models
gpu_memory_fraction=0.7

# Enhanced memory management
vmargs=-Dlog4j2.formatMsgNoLookups=true -Xmx8g -XX:MaxDirectMemorySize=4g -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=100

# =============================================================================
# Model Store Configuration
# =============================================================================
model_store=/models
load_models=all

# Model snapshot for automatic loading
model_snapshot={"name":"rtx4090-startup.cfg","modelCount":3,"models":{"stable-diffusion":{"1.0":{"defaultVersion":true,"marName":"stable-diffusion.mar","minWorkers":2,"maxWorkers":6,"batchSize":8,"maxBatchDelay":100,"responseTimeout":600}},"llama-7b":{"1.0":{"defaultVersion":true,"marName":"llama-7b.mar","minWorkers":1,"maxWorkers":4,"batchSize":4,"maxBatchDelay":200,"responseTimeout":1200}},"resnet-50":{"1.0":{"defaultVersion":true,"marName":"resnet-50.mar","minWorkers":2,"maxWorkers":8,"batchSize":32,"maxBatchDelay":50,"responseTimeout":120}}}}

# =============================================================================
# API and Security Configuration
# =============================================================================
# CORS settings for web integration
cors_allowed_origin=*
cors_allowed_methods=GET,POST,PUT,DELETE,OPTIONS
cors_allowed_headers=*

# API limits for cloud deployment
max_request_size=104857600   # 100MB for large models/images
max_response_size=104857600  # 100MB for generated content

# =============================================================================
# Logging and Monitoring
# =============================================================================
# Metrics configuration
metrics_config=/config/metrics.yaml
enable_metrics_api=true
metrics_mode=prometheus

# Logging for vast.ai environment
default_response_timeout=300
install_py_dep_per_model=true
model_server_home=/tmp/torchserve

# =============================================================================
# RTX 4090 Specific Optimizations
# =============================================================================
# Enable mixed precision for better performance
enable_envvars_config=true

# Async logging for performance
async_logging=true

# Prefer direct buffer for GPU memory
prefer_direct_buffer=true

# Enable model versioning
enable_model_api=true

# Workflow configuration for complex pipelines
workflow_store=/models/workflows
