apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ai-inference
    app.kubernetes.io/component: service
    app.kubernetes.io/name: ai-inference
  name: ai-inference
  namespace: gameforge
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ai-inference
  template:
    metadata:
      labels:
        app: ai-inference
    spec:
      containers:
      - env: []
        image: nvidia/cuda:12.1-runtime-alpine
        name: ai-inference
        ports:
        - containerPort: 50051
          name: grpc
        - containerPort: 8001
          name: http
        resources:
          limits:
            cpu: 4000m
            memory: 16Gi
            nvidia.com/gpu: 1
          requests:
            cpu: 1000m
            memory: 4Gi
            nvidia.com/gpu: 1
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
        volumeMounts:
        - mountPath: /app/models
          name: models
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsUser: 1001
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: ai-inference-models
