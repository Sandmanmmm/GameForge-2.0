apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: ai-inference
    app.kubernetes.io/component: gpu-service
    app.kubernetes.io/name: ai-inference
  name: ai-inference
  namespace: gameforge-prod
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ai-inference
  template:
    metadata:
      labels:
        app: ai-inference
    spec:
      containers:
      - env:
        - name: CUDA_VISIBLE_DEVICES
          value: '0'
        - name: MODEL_PATH
          value: /models
        image: ${registry}/ai-inference-hardened
        livenessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 30
          periodSeconds: 60
        name: ai-inference
        ports:
        - containerPort: 8001
          name: http
        readinessProbe:
          httpGet:
            path: /health
            port: 8001
          initialDelaySeconds: 30
          periodSeconds: 60
        resources:
          limits:
            cpu: 2000m
            memory: 8Gi
            nvidia.com/gpu: 1
          requests:
            cpu: 500m
            memory: 2Gi
            nvidia.com/gpu: 1
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
          runAsNonRoot: true
        volumeMounts:
        - mountPath: /models
          name: models
        - mountPath: /tmp
          name: tmp
      securityContext:
        fsGroup: 1001
        runAsGroup: 1001
        runAsUser: 1001
      volumes:
      - name: models
        persistentVolumeClaim:
          claimName: ai-inference-models
      - emptyDir: {}
        name: tmp
