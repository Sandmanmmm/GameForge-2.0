# Docker Compose for Vast.ai Deployment
version: '3.8'

services:
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3

  gameforge-sdxl:
    build: .
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./outputs:/app/outputs
      - ./models:/app/models
      - model_cache:/tmp/models
    environment:
      # GPU Configuration
      - CUDA_VISIBLE_DEVICES=0
      - DEVICE=cuda
      
      # Model Configuration  
      - BASE_MODEL_PATH=segmind/SSD-1B
      - MODEL_CACHE_DIR=/tmp/models
      
      # Service Configuration
      - HOST=0.0.0.0
      - PORT=8000
      - DEBUG=false
      
      # Storage
      - OUTPUT_DIR=/app/outputs
      - TEMP_DIR=/tmp
      
      # Redis Connection
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      
      # Performance Tuning for RTX 4090
      - MAX_CACHED_MODELS=3
      - MAX_BATCH_SIZE=6
      - ENABLE_ATTENTION_SLICING=false
      - ENABLE_CPU_OFFLOAD=false
      
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

volumes:
  redis_data:
  model_cache:
