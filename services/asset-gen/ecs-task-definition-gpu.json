{
  "family": "gameforge-sdxl-production-task",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["EC2"],
  "cpu": "4096",
  "memory": "16384",
  "executionRoleArn": "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
  "taskRoleArn": "arn:aws:iam::927588814706:role/ecsTaskRole",
  "containerDefinitions": [
    {
      "name": "sdxl-production-service",
      "image": "pytorch/pytorch:2.1.0-cuda12.1-cudnn8-devel",
      "essential": true,
      "resourceRequirements": [
        {
          "type": "GPU",
          "value": "1"
        }
      ],
      "portMappings": [
        {
          "containerPort": 8080,
          "protocol": "tcp",
          "name": "sdxl-port"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/gameforge-sdxl-production",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "environment": [
        {
          "name": "PORT",
          "value": "8080"
        },
        {
          "name": "PYTHON_ENV",
          "value": "production"
        },
        {
          "name": "CUDA_VISIBLE_DEVICES",
          "value": "0"
        },
        {
          "name": "TORCH_CACHE",
          "value": "/tmp/torch_cache"
        },
        {
          "name": "HF_HOME",
          "value": "/tmp/huggingface_cache"
        },
        {
          "name": "TRANSFORMERS_CACHE",
          "value": "/tmp/transformers_cache"
        }
      ],
      "mountPoints": [
        {
          "sourceVolume": "model-cache",
          "containerPath": "/tmp/model_cache",
          "readOnly": false
        }
      ],
      "command": [
        "bash",
        "-c",
        "echo 'Installing system dependencies...' && apt-get update && apt-get install -y curl git && echo 'Installing Python packages...' && pip install --no-cache-dir fastapi uvicorn[standard] pillow pydantic diffusers transformers accelerate xformers safetensors && echo 'Copying optimized service...' && cat > /tmp/sdxl_service.py << 'PYTHON_SERVICE_EOF'\nimport os\nimport io\nimport base64\nimport asyncio\nimport logging\nfrom typing import Optional, Dict, Any\nfrom contextlib import asynccontextmanager\n\nimport torch\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport uvicorn\nfrom diffusers import StableDiffusionXLPipeline, StableDiffusionXLImg2ImgPipeline\nimport gc\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nMODEL_CACHE: Dict[str, Any] = {}\nMODEL_ID = \"stabilityai/stable-diffusion-xl-base-1.0\"\nREFINER_ID = \"stabilityai/stable-diffusion-xl-refiner-1.0\"\n\nclass ImageRequest(BaseModel):\n    prompt: str = Field(..., description=\"Text prompt for image generation\")\n    negative_prompt: Optional[str] = Field(None, description=\"Negative prompt\")\n    width: Optional[int] = Field(1024, ge=256, le=1536)\n    height: Optional[int] = Field(1024, ge=256, le=1536)\n    steps: Optional[int] = Field(25, ge=10, le=50)\n    guidance_scale: Optional[float] = Field(7.5, ge=1.0, le=20.0)\n    seed: Optional[int] = Field(None, description=\"Random seed\")\n\nclass ImageResponse(BaseModel):\n    image: str\n    metadata: Dict[str, Any]\n\ndef get_device_info():\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        memory_allocated = torch.cuda.memory_allocated() / 1024**3\n        memory_reserved = torch.cuda.memory_reserved() / 1024**3\n        memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3\n        return {\n            \"device\": device,\n            \"memory\": {\n                \"allocated_gb\": round(memory_allocated, 2),\n                \"reserved_gb\": round(memory_reserved, 2),\n                \"total_gb\": round(memory_total, 2),\n                \"free_gb\": round(memory_total - memory_reserved, 2)\n            }\n        }\n    else:\n        return {\"device\": \"cpu\", \"memory\": {\"allocated_gb\": 0, \"reserved_gb\": 0, \"total_gb\": 0, \"free_gb\": 0}}\n\nasync def load_sdxl_models():\n    global MODEL_CACHE\n    if \"base_pipeline\" in MODEL_CACHE:\n        logger.info(\"Models already loaded from cache\")\n        return\n    \n    logger.info(\"Loading SDXL base model...\")\n    try:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        dtype = torch.float16 if device == \"cuda\" else torch.float32\n        logger.info(f\"Using device: {device}, dtype: {dtype}\")\n        \n        base_pipeline = StableDiffusionXLPipeline.from_pretrained(\n            MODEL_ID,\n            torch_dtype=dtype,\n            use_safetensors=True,\n            variant=\"fp16\" if device == \"cuda\" else None\n        )\n        \n        optimizations = {\"fp16\": False, \"xformers\": False, \"cpu_offload\": False}\n        \n        if device == \"cuda\":\n            base_pipeline = base_pipeline.to(device, dtype=dtype)\n            optimizations[\"fp16\"] = True\n            \n            try:\n                base_pipeline.enable_xformers_memory_efficient_attention()\n                optimizations[\"xformers\"] = True\n                logger.info(\"‚úÖ xFormers enabled\")\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è xFormers not available: {e}\")\n            \n            try:\n                if torch.cuda.get_device_properties(0).total_memory < 16 * 1024**3:\n                    base_pipeline.enable_model_cpu_offload()\n                    optimizations[\"cpu_offload\"] = True\n                    logger.info(\"‚úÖ CPU offload enabled\")\n            except Exception as e:\n                logger.warning(f\"‚ö†Ô∏è CPU offload failed: {e}\")\n        else:\n            base_pipeline = base_pipeline.to(device)\n        \n        MODEL_CACHE[\"base_pipeline\"] = base_pipeline\n        MODEL_CACHE[\"device\"] = device\n        MODEL_CACHE[\"dtype\"] = dtype\n        MODEL_CACHE[\"optimizations\"] = optimizations\n        \n        logger.info(\"‚úÖ SDXL base model loaded successfully\")\n        device_info = get_device_info()\n        logger.info(f\"Memory usage: {device_info['memory']}\")\n        \n    except Exception as e:\n        logger.error(f\"‚ùå Failed to load SDXL models: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Model loading failed: {str(e)}\")\n\ndef cleanup_memory():\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    logger.info(\"üöÄ Starting GameForge SDXL Production Service...\")\n    await load_sdxl_models()\n    yield\n    logger.info(\"üí§ Shutting down service...\")\n    cleanup_memory()\n\napp = FastAPI(\n    title=\"GameForge SDXL Production Service\",\n    version=\"2.0.0\",\n    description=\"Production SDXL with GPU acceleration and model caching\",\n    lifespan=lifespan\n)\n\n@app.get(\"/health\")\nasync def health():\n    device_info = get_device_info()\n    models_loaded = len(MODEL_CACHE) > 0\n    return {\n        \"status\": \"healthy\" if models_loaded else \"loading\",\n        \"version\": \"2.0.0\",\n        \"service\": \"sdxl-production\",\n        \"models_loaded\": models_loaded,\n        \"device\": device_info[\"device\"],\n        \"memory\": device_info[\"memory\"]\n    }\n\n@app.post(\"/generate\", response_model=ImageResponse)\nasync def generate_image(request: ImageRequest, background_tasks: BackgroundTasks):\n    if \"base_pipeline\" not in MODEL_CACHE:\n        raise HTTPException(status_code=503, detail=\"SDXL model not loaded yet\")\n    \n    try:\n        pipeline = MODEL_CACHE[\"base_pipeline\"]\n        device = MODEL_CACHE[\"device\"]\n        \n        generator = None\n        if request.seed is not None:\n            generator = torch.Generator(device=device).manual_seed(request.seed)\n        \n        logger.info(f\"Generating: '{request.prompt[:50]}...' ({request.width}x{request.height})\")\n        \n        with torch.inference_mode():\n            result = pipeline(\n                prompt=request.prompt,\n                negative_prompt=request.negative_prompt,\n                width=request.width,\n                height=request.height,\n                num_inference_steps=request.steps,\n                guidance_scale=request.guidance_scale,\n                generator=generator,\n                output_type=\"pil\"\n            )\n        \n        image = result.images[0]\n        \n        buffer = io.BytesIO()\n        image.save(buffer, format=\"PNG\", quality=95)\n        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n        \n        background_tasks.add_task(cleanup_memory)\n        \n        metadata = {\n            \"prompt\": request.prompt,\n            \"negative_prompt\": request.negative_prompt,\n            \"width\": request.width,\n            \"height\": request.height,\n            \"steps\": request.steps,\n            \"guidance_scale\": request.guidance_scale,\n            \"seed\": request.seed,\n            \"model\": MODEL_ID,\n            \"device\": device,\n            \"optimizations\": MODEL_CACHE.get(\"optimizations\", {}),\n            \"format\": \"PNG\"\n        }\n        \n        logger.info(f\"‚úÖ Generated successfully ({len(img_base64)} chars)\")\n        \n        return ImageResponse(image=img_base64, metadata=metadata)\n        \n    except Exception as e:\n        logger.error(f\"‚ùå Generation failed: {e}\")\n        cleanup_memory()\n        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n\nif __name__ == \"__main__\":\n    port = int(os.getenv(\"PORT\", 8080))\n    uvicorn.run(app, host=\"0.0.0.0\", port=port, access_log=True, log_level=\"info\")\nPYTHON_SERVICE_EOF\necho 'Starting SDXL Production Service...' && python /tmp/sdxl_service.py"
      ],
      "healthCheck": {
        "command": ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"],
        "interval": 60,
        "timeout": 30,
        "retries": 3,
        "startPeriod": 300
      },
      "stopTimeout": 120
    }
  ],
  "volumes": [
    {
      "name": "model-cache",
      "host": {
        "sourcePath": "/tmp/model_cache"
      }
    }
  ],
  "placementConstraints": [
    {
      "type": "memberOf",
      "expression": "attribute:ecs.instance-type =~ g4dn.*"
    }
  ]
}
