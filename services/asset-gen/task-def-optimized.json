{
  "family": "gameforge-sdxl-optimized",
  "taskRoleArn": "arn:aws:iam::927588814706:role/ecsTaskRole",
  "executionRoleArn": "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "4096",
  "memory": "16384",
  "containerDefinitions": [
    {
      "name": "sdxl-optimized",
      "image": "public.ecr.aws/docker/library/python:3.11-slim",
      "essential": true,
      "portMappings": [
        {
          "containerPort": 8080,
          "protocol": "tcp"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/gameforge-sdxl-optimized",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "environment": [
        {
          "name": "PORT",
          "value": "8080"
        },
        {
          "name": "HF_HOME", 
          "value": "/tmp/huggingface_cache"
        },
        {
          "name": "TRANSFORMERS_CACHE",
          "value": "/tmp/transformers_cache"
        }
      ],
      "command": [
        "bash",
        "-c",
        "apt-get update && apt-get install -y curl git build-essential && pip install --no-cache-dir 'torch>=2.0.0' torchvision --index-url https://download.pytorch.org/whl/cpu && pip install --no-cache-dir fastapi 'uvicorn[standard]' pillow pydantic 'diffusers>=0.21.0' 'transformers>=4.25.0' accelerate safetensors compel psutil && python -c \"import os, io, base64, asyncio, logging, gc; from typing import Optional, Dict, Any; from contextlib import asynccontextmanager; import torch; from fastapi import FastAPI, HTTPException, BackgroundTasks; from pydantic import BaseModel, Field; import uvicorn; from diffusers import DiffusionPipeline; logging.basicConfig(level=logging.INFO); logger = logging.getLogger(__name__); MODEL_CACHE = {}; MODEL_ID = 'segmind/SSD-1B'; class ImageRequest(BaseModel): prompt: str = Field(..., min_length=1, max_length=500); negative_prompt: Optional[str] = Field(None, max_length=500); width: Optional[int] = Field(512, ge=256, le=1024); height: Optional[int] = Field(512, ge=256, le=1024); steps: Optional[int] = Field(20, ge=10, le=30); guidance_scale: Optional[float] = Field(7.5, ge=1.0, le=15.0); seed: Optional[int] = Field(None, ge=0, le=2147483647); class ImageResponse(BaseModel): image: str; metadata: Dict[str, Any]; class ModelStatus(BaseModel): loaded: bool; model_id: str; device: str; memory_usage: Dict[str, Any]; optimizations: Dict[str, bool]; def get_memory_info(): try: import psutil; memory = psutil.virtual_memory(); return {'total_gb': round(memory.total / 1024**3, 2), 'available_gb': round(memory.available / 1024**3, 2), 'used_gb': round(memory.used / 1024**3, 2), 'percent': memory.percent}; except ImportError: return {'total_gb': 0, 'available_gb': 0, 'used_gb': 0, 'percent': 0}; async def load_optimized_model(): global MODEL_CACHE; if 'pipeline' in MODEL_CACHE: logger.info('Model already loaded from cache'); return; logger.info(f'Loading optimized model: {MODEL_ID}...'); try: pipeline = DiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float32, use_safetensors=True, safety_checker=None, requires_safety_checker=False); pipeline = pipeline.to('cpu'); optimizations = {'memory_efficient_attention': False, 'cpu_offload': True, 'safety_checker_disabled': True, 'torch_compile': False}; try: pipeline.enable_attention_slicing(); optimizations['attention_slicing'] = True; logger.info(' Attention slicing enabled'); except Exception as e: logger.warning(f' Attention slicing failed: {e}'); optimizations['attention_slicing'] = False; MODEL_CACHE['pipeline'] = pipeline; MODEL_CACHE['model_id'] = MODEL_ID; MODEL_CACHE['device'] = 'cpu'; MODEL_CACHE['optimizations'] = optimizations; logger.info(' Optimized model loaded successfully'); memory_info = get_memory_info(); logger.info(f'Memory usage: {memory_info}'); except Exception as e: logger.error(f' Failed to load model: {e}'); raise; def cleanup_memory(): gc.collect(); @asynccontextmanager; async def lifespan(app: FastAPI): logger.info(' Starting GameForge Optimized SDXL Service...'); await load_optimized_model(); yield; logger.info(' Shutting down service...'); cleanup_memory(); app = FastAPI(title='GameForge SDXL Optimized Service', version='2.1.0', description='CPU-optimized SDXL service with model caching', lifespan=lifespan); @app.get('/health'); async def health(): memory_info = get_memory_info(); models_loaded = 'pipeline' in MODEL_CACHE; return {'status': 'healthy' if models_loaded else 'loading', 'version': '2.1.0', 'service': 'sdxl-optimized', 'models_loaded': models_loaded, 'device': 'cpu', 'memory': memory_info}; @app.get('/model-status', response_model=ModelStatus); async def get_model_status(): if 'pipeline' not in MODEL_CACHE: raise HTTPException(status_code=503, detail='Model not loaded yet'); memory_info = get_memory_info(); return ModelStatus(loaded=True, model_id=MODEL_CACHE['model_id'], device=MODEL_CACHE['device'], memory_usage=memory_info, optimizations=MODEL_CACHE.get('optimizations', {})); @app.post('/generate', response_model=ImageResponse); async def generate_image(request: ImageRequest, background_tasks: BackgroundTasks): if 'pipeline' not in MODEL_CACHE: raise HTTPException(status_code=503, detail='Model not loaded yet'); try: pipeline = MODEL_CACHE['pipeline']; generator = torch.Generator('cpu').manual_seed(request.seed) if request.seed is not None else None; logger.info(f'Generating: {request.prompt[:50]}... ({request.width}x{request.height}, {request.steps} steps)'); with torch.inference_mode(): result = pipeline(prompt=request.prompt, negative_prompt=request.negative_prompt, width=request.width, height=request.height, num_inference_steps=request.steps, guidance_scale=request.guidance_scale, generator=generator, output_type='pil'); image = result.images[0]; buffer = io.BytesIO(); image.save(buffer, format='PNG', optimize=True); img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8'); background_tasks.add_task(cleanup_memory); metadata = {'prompt': request.prompt, 'negative_prompt': request.negative_prompt, 'width': request.width, 'height': request.height, 'steps': request.steps, 'guidance_scale': request.guidance_scale, 'seed': request.seed, 'model': MODEL_CACHE['model_id'], 'device': 'cpu', 'optimizations': MODEL_CACHE.get('optimizations', {}), 'format': 'PNG', 'inference_type': 'cpu-optimized'}; logger.info(f' Generated successfully ({len(img_base64)} chars)'); return ImageResponse(image=img_base64, metadata=metadata); except Exception as e: logger.error(f' Generation failed: {e}'); cleanup_memory(); raise HTTPException(status_code=500, detail=f'Generation failed: {str(e)}'); @app.post('/reload-model'); async def reload_model(): global MODEL_CACHE; logger.info('Reloading model...'); MODEL_CACHE.clear(); cleanup_memory(); await load_optimized_model(); return {'status': 'success', 'message': 'Model reloaded'}; port = int(os.getenv('PORT', 8080)); logger.info(f'Starting optimized SDXL service on port {port}'); uvicorn.run(app, host='0.0.0.0', port=port, access_log=True, log_level='info')\""
      ],
      "healthCheck": {
        "command": [
          "CMD-SHELL",
          "curl -f http://localhost:8080/health || exit 1"
        ],
        "interval": 60,
        "timeout": 10,
        "retries": 3,
        "startPeriod": 300
      },
      "stopTimeout": 60
    }
  ]
}