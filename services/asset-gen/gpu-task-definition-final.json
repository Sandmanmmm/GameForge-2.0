{
    "family": "gameforge-sdxl-gpu-task",
    "requiresCompatibilities": ["EC2"],
    "networkMode": "awsvpc",
    "cpu": "2048",
    "memory": "14336",
    "executionRoleArn": "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
    "taskRoleArn": "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
    "containerDefinitions": [
  {
    "name": "sdxl-gpu-service",
    "image": "nvidia/cuda:11.8-devel-ubuntu20.04",
    "cpu": 0,
    "memory": 14000,
    "memoryReservation": 12000,
    "portMappings": [
      {
        "containerPort": 8080,
        "hostPort": 8080,
        "protocol": "tcp",
        "name": "sdxl-gpu-port"
      }
    ],
    "essential": true,
    "resourceRequirements": [
      {
        "type": "GPU",
        "value": "1"
      }
    ],
    "command": [
      "bash",
      "-c",
      "apt-get update && apt-get install -y curl python3 python3-pip && python3 -m pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cu118 && python3 -m pip install --no-cache-dir fastapi uvicorn pillow pydantic diffusers transformers accelerate safetensors xformers && cat > /tmp/sdxl_gpu_optimized.py << \"PYTHON_EOF\" && python3 /tmp/sdxl_gpu_optimized.py\nfrom fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom PIL import Image, ImageDraw\nimport torch, io, base64, os, uvicorn, logging\nfrom diffusers import DiffusionPipeline\nimport gc\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\nMODEL_CACHE = {}\n\nclass ImageRequest(BaseModel):\n    prompt: str\n    width: int = 1024\n    height: int = 1024\n    steps: int = 25\n    guidance_scale: float = 7.5\n\nclass ImageResponse(BaseModel):\n    image: str\n    metadata: dict\n\napp = FastAPI(title=\"GameForge SDXL GPU Optimized\", version=\"3.0.0\")\n\n@app.on_event(\"startup\")\nasync def load_model():\n    global MODEL_CACHE\n    try:\n        if not torch.cuda.is_available():\n            raise Exception(\"CUDA not available\")\n        \n        logger.info(f\"GPU Available: {torch.cuda.get_device_name(0)}\")\n        logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory // 1024**3}GB\")\n        \n        logger.info(\"Loading SDXL GPU model...\")\n        pipeline = DiffusionPipeline.from_pretrained(\n            \"segmind/SSD-1B\", \n            torch_dtype=torch.float16,\n            safety_checker=None,\n            requires_safety_checker=False,\n            variant=\"fp16\"\n        )\n        \n        pipeline = pipeline.to(\"cuda\")\n        \n        # GPU Optimizations\n        try:\n            pipeline.enable_xformers_memory_efficient_attention()\n            logger.info(\"âœ… xFormers enabled\")\n        except:\n            logger.warning(\"xFormers not available, using default attention\")\n        \n        pipeline.enable_model_cpu_offload()\n        logger.info(\"âœ… CPU offload enabled\")\n        \n        # Pre-compile for faster inference\n        pipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n        logger.info(\"âœ… Model compiled for optimization\")\n        \n        MODEL_CACHE[\"pipeline\"] = pipeline\n        MODEL_CACHE[\"device\"] = \"cuda\"\n        \n        # Memory management\n        torch.cuda.empty_cache()\n        gc.collect()\n        \n        logger.info(\"ðŸš€ SDXL GPU model loaded with ALL optimizations!\")\n        \n    except Exception as e:\n        logger.error(f\"GPU model loading failed: {e}\")\n        logger.info(\"Fallback to CPU mode\")\n        try:\n            pipeline = DiffusionPipeline.from_pretrained(\n                \"segmind/SSD-1B\",\n                torch_dtype=torch.float32,\n                safety_checker=None,\n                requires_safety_checker=False\n            )\n            pipeline = pipeline.to(\"cpu\")\n            pipeline.enable_attention_slicing()\n            MODEL_CACHE[\"pipeline\"] = pipeline\n            MODEL_CACHE[\"device\"] = \"cpu\"\n            logger.info(\"âœ… CPU fallback mode active\")\n        except Exception as cpu_error:\n            logger.error(f\"CPU fallback also failed: {cpu_error}\")\n            MODEL_CACHE[\"fallback\"] = True\n\n@app.get(\"/health\")\ndef health():\n    gpu_available = torch.cuda.is_available() if torch.cuda else False\n    return {\n        \"status\": \"healthy\",\n        \"version\": \"3.0.0\",\n        \"service\": \"sdxl-gpu-optimized\",\n        \"model_loaded\": \"pipeline\" in MODEL_CACHE,\n        \"device\": MODEL_CACHE.get(\"device\", \"unknown\"),\n        \"gpu_available\": gpu_available,\n        \"gpu_name\": torch.cuda.get_device_name(0) if gpu_available else \"N/A\",\n        \"fallback_mode\": \"fallback\" in MODEL_CACHE\n    }\n\n@app.get(\"/model-status\")\ndef model_status():\n    if \"pipeline\" in MODEL_CACHE:\n        device = MODEL_CACHE.get(\"device\", \"unknown\")\n        optimizations = [\"model_caching\"]\n        \n        if device == \"cuda\":\n            optimizations.extend([\"fp16\", \"xformers\", \"cpu_offload\", \"torch_compile\"])\n        elif device == \"cpu\":\n            optimizations.append(\"attention_slicing\")\n            \n        return {\n            \"loaded\": True,\n            \"model\": \"segmind/SSD-1B\",\n            \"device\": device,\n            \"optimizations\": optimizations,\n            \"gpu_memory_used\": f\"{torch.cuda.memory_allocated() // 1024**2}MB\" if torch.cuda.is_available() else \"N/A\"\n        }\n    return {\"loaded\": False, \"fallback_mode\": True}\n\n@app.get(\"/gpu-stats\")\ndef gpu_stats():\n    if torch.cuda.is_available():\n        return {\n            \"gpu_name\": torch.cuda.get_device_name(0),\n            \"gpu_memory_total\": f\"{torch.cuda.get_device_properties(0).total_memory // 1024**3}GB\",\n            \"gpu_memory_allocated\": f\"{torch.cuda.memory_allocated() // 1024**2}MB\",\n            \"gpu_memory_cached\": f\"{torch.cuda.memory_reserved() // 1024**2}MB\",\n            \"gpu_utilization\": \"Available\"\n        }\n    return {\"gpu_available\": False}\n\n@app.post(\"/generate\", response_model=ImageResponse)\ndef generate_image(request: ImageRequest):\n    if \"pipeline\" in MODEL_CACHE:\n        try:\n            pipeline = MODEL_CACHE[\"pipeline\"]\n            device = MODEL_CACHE.get(\"device\", \"cpu\")\n            \n            logger.info(f\"Generating with SDXL on {device.upper()}: {request.prompt[:50]}...\")\n            \n            # GPU memory management\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n            \n            with torch.inference_mode():\n                result = pipeline(\n                    prompt=request.prompt,\n                    width=request.width,\n                    height=request.height,\n                    num_inference_steps=request.steps,\n                    guidance_scale=request.guidance_scale,\n                    output_type=\"pil\"\n                )\n            \n            image = result.images[0]\n            buffer = io.BytesIO()\n            image.save(buffer, format=\"PNG\", optimize=True)\n            img_base64 = base64.b64encode(buffer.getvalue()).decode()\n            \n            # Memory cleanup\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n            gc.collect()\n            \n            return ImageResponse(\n                image=img_base64,\n                metadata={\n                    \"prompt\": request.prompt,\n                    \"width\": request.width,\n                    \"height\": request.height,\n                    \"steps\": request.steps,\n                    \"model\": \"segmind/SSD-1B\",\n                    \"device\": device,\n                    \"service\": \"sdxl-gpu-optimized\",\n                    \"format\": \"PNG\",\n                    \"cached\": True,\n                    \"optimizations\": \"fp16+xformers+compile\" if device == \"cuda\" else \"cpu_slicing\"\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f\"SDXL generation failed: {e}\")\n            # Cleanup on error\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n            gc.collect()\n    \n    # Fallback to placeholder\n    logger.info(\"Using placeholder generation\")\n    img = Image.new(\"RGB\", (request.width, request.height), color=\"lightgreen\")\n    draw = ImageDraw.Draw(img)\n    text = f\"GPU Fallback: {request.prompt[:30]}\"\n    draw.text((10, 10), text, fill=\"darkgreen\")\n    \n    buffer = io.BytesIO()\n    img.save(buffer, format=\"PNG\")\n    img_base64 = base64.b64encode(buffer.getvalue()).decode()\n    \n    return ImageResponse(\n        image=img_base64,\n        metadata={\n            \"prompt\": request.prompt,\n            \"width\": request.width,\n            \"height\": request.height,\n            \"format\": \"PNG\",\n            \"service\": \"gpu-placeholder-fallback\"\n        }\n    )\n\nif __name__ == \"__main__\":\n    port = int(os.getenv(\"PORT\", 8080))\n    uvicorn.run(app, host=\"0.0.0.0\", port=port, workers=1)\nPYTHON_EOF"
    ],
    "environment": [
      {
        "name": "PORT",
        "value": "8080"
      },
      {
        "name": "PYTHON_ENV",
        "value": "production"
      },
      {
        "name": "CUDA_VISIBLE_DEVICES",
        "value": "0"
      },
      {
        "name": "HF_HOME",
        "value": "/tmp/huggingface_cache"
      },
      {
        "name": "PYTORCH_CUDA_ALLOC_CONF",
        "value": "max_split_size_mb:128"
      }
    ],
    "logConfiguration": {
      "logDriver": "awslogs",
      "options": {
        "awslogs-group": "/ecs/gameforge-sdxl-gpu",
        "awslogs-region": "us-east-1",
        "awslogs-stream-prefix": "ecs"
      }
    },
    "healthCheck": {
      "command": [
        "CMD-SHELL",
        "curl -f http://localhost:8080/health || exit 1"
      ],
      "interval": 60,
      "timeout": 15,
      "retries": 3,
      "startPeriod": 600
    }
  }
]

}
