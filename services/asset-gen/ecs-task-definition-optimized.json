{
  "family": "gameforge-sdxl-optimized-task",
  "networkMode": "awsvpc",
  "requiresCompatibilities": ["FARGATE"],
  "cpu": "4096",
  "memory": "16384",
  "executionRoleArn": "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
  "taskRoleArn": "arn:aws:iam::927588814706:role/ecsTaskRole",
  "containerDefinitions": [
    {
      "name": "sdxl-optimized-service",
      "image": "python:3.11-slim",
      "essential": true,
      "portMappings": [
        {
          "containerPort": 8080,
          "protocol": "tcp",
          "name": "sdxl-port"
        }
      ],
      "logConfiguration": {
        "logDriver": "awslogs",
        "options": {
          "awslogs-group": "/ecs/gameforge-sdxl-optimized",
          "awslogs-region": "us-east-1",
          "awslogs-stream-prefix": "ecs"
        }
      },
      "environment": [
        {
          "name": "PORT",
          "value": "8080"
        },
        {
          "name": "PYTHON_ENV",
          "value": "production"
        },
        {
          "name": "HF_HOME",
          "value": "/tmp/huggingface_cache"
        },
        {
          "name": "TRANSFORMERS_CACHE",
          "value": "/tmp/transformers_cache"
        }
      ],
      "command": [
        "bash",
        "-c",
        "echo 'Installing system dependencies...' && apt-get update && apt-get install -y curl git build-essential && echo 'Installing Python packages with caching optimization...' && pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cpu && pip install --no-cache-dir fastapi uvicorn[standard] pillow pydantic diffusers transformers accelerate safetensors compel && echo 'Creating optimized SDXL service...' && cat > /tmp/sdxl_service.py << 'PYTHON_SERVICE_EOF'\nimport os\nimport io\nimport base64\nimport asyncio\nimport logging\nimport gc\nfrom typing import Optional, Dict, Any\nfrom contextlib import asynccontextmanager\n\nimport torch\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport uvicorn\nfrom diffusers import StableDiffusionXLPipeline, DiffusionPipeline\nfrom diffusers.optimization import enable_model_cpu_offload\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Global model cache\nMODEL_CACHE: Dict[str, Any] = {}\n\n# Use a smaller, faster SDXL model for CPU inference\nMODEL_ID = \"segmind/SSD-1B\"  # Smaller SDXL-based model, faster inference\n\nclass ImageRequest(BaseModel):\n    prompt: str = Field(..., min_length=1, max_length=500, description=\"Text prompt for image generation\")\n    negative_prompt: Optional[str] = Field(None, max_length=500, description=\"Negative prompt\")\n    width: Optional[int] = Field(512, ge=256, le=1024, description=\"Image width\")\n    height: Optional[int] = Field(512, ge=256, le=1024, description=\"Image height\")\n    steps: Optional[int] = Field(20, ge=10, le=30, description=\"Number of steps (limited for CPU)\")\n    guidance_scale: Optional[float] = Field(7.5, ge=1.0, le=15.0, description=\"Guidance scale\")\n    seed: Optional[int] = Field(None, ge=0, le=2147483647, description=\"Random seed\")\n\nclass ImageResponse(BaseModel):\n    image: str = Field(..., description=\"Base64 encoded PNG image\")\n    metadata: Dict[str, Any] = Field(..., description=\"Generation metadata\")\n\nclass ModelStatus(BaseModel):\n    loaded: bool\n    model_id: str\n    device: str\n    memory_usage: Dict[str, float]\n    optimizations: Dict[str, bool]\n\ndef get_memory_info():\n    \"\"\"Get memory usage information\"\"\"\n    try:\n        import psutil\n        memory = psutil.virtual_memory()\n        return {\n            \"total_gb\": round(memory.total / 1024**3, 2),\n            \"available_gb\": round(memory.available / 1024**3, 2),\n            \"used_gb\": round(memory.used / 1024**3, 2),\n            \"percent\": memory.percent\n        }\n    except ImportError:\n        return {\"total_gb\": 0, \"available_gb\": 0, \"used_gb\": 0, \"percent\": 0}\n\nasync def load_optimized_model():\n    \"\"\"Load optimized SDXL model with CPU optimizations\"\"\"\n    global MODEL_CACHE\n    \n    if \"pipeline\" in MODEL_CACHE:\n        logger.info(\"Model already loaded from cache\")\n        return\n    \n    logger.info(f\"Loading optimized model: {MODEL_ID}...\")\n    \n    try:\n        # Load with CPU optimizations\n        pipeline = DiffusionPipeline.from_pretrained(\n            MODEL_ID,\n            torch_dtype=torch.float32,  # Use float32 for CPU\n            use_safetensors=True,\n            safety_checker=None,  # Disable safety checker for faster inference\n            requires_safety_checker=False\n        )\n        \n        # CPU optimizations\n        pipeline = pipeline.to(\"cpu\")\n        \n        # Enable memory efficient attention if available\n        optimizations = {\n            \"memory_efficient_attention\": False,\n            \"cpu_offload\": True,\n            \"safety_checker_disabled\": True,\n            \"torch_compile\": False\n        }\n        \n        try:\n            pipeline.enable_attention_slicing()\n            optimizations[\"attention_slicing\"] = True\n            logger.info(\"✅ Attention slicing enabled\")\n        except Exception as e:\n            logger.warning(f\"⚠️ Attention slicing failed: {e}\")\n            optimizations[\"attention_slicing\"] = False\n        \n        # Try torch compile for faster inference (PyTorch 2.0+)\n        try:\n            if hasattr(torch, \"compile\"):\n                pipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n                optimizations[\"torch_compile\"] = True\n                logger.info(\"✅ Torch compile enabled\")\n        except Exception as e:\n            logger.warning(f\"⚠️ Torch compile failed: {e}\")\n        \n        MODEL_CACHE[\"pipeline\"] = pipeline\n        MODEL_CACHE[\"model_id\"] = MODEL_ID\n        MODEL_CACHE[\"device\"] = \"cpu\"\n        MODEL_CACHE[\"optimizations\"] = optimizations\n        \n        logger.info(\"✅ Optimized model loaded successfully\")\n        \n        # Log memory usage\n        memory_info = get_memory_info()\n        logger.info(f\"Memory usage: {memory_info}\")\n        \n    except Exception as e:\n        logger.error(f\"❌ Failed to load model: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Model loading failed: {str(e)}\")\n\ndef cleanup_memory():\n    \"\"\"Clean up memory\"\"\"\n    gc.collect()\n    if torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    \"\"\"Application lifespan management\"\"\"\n    logger.info(\"🚀 Starting GameForge Optimized SDXL Service...\")\n    await load_optimized_model()\n    yield\n    logger.info(\"💤 Shutting down service...\")\n    cleanup_memory()\n\n# Create FastAPI app\napp = FastAPI(\n    title=\"GameForge SDXL Optimized Service\",\n    version=\"2.1.0\",\n    description=\"CPU-optimized SDXL service with model caching and fast inference\",\n    lifespan=lifespan\n)\n\n@app.get(\"/health\")\nasync def health():\n    \"\"\"Health check endpoint\"\"\"\n    memory_info = get_memory_info()\n    models_loaded = \"pipeline\" in MODEL_CACHE\n    \n    return {\n        \"status\": \"healthy\" if models_loaded else \"loading\",\n        \"version\": \"2.1.0\",\n        \"service\": \"sdxl-optimized\",\n        \"models_loaded\": models_loaded,\n        \"device\": \"cpu\",\n        \"memory\": memory_info\n    }\n\n@app.get(\"/model-status\", response_model=ModelStatus)\nasync def get_model_status():\n    \"\"\"Get detailed model status\"\"\"\n    if \"pipeline\" not in MODEL_CACHE:\n        raise HTTPException(status_code=503, detail=\"Model not loaded yet\")\n    \n    memory_info = get_memory_info()\n    \n    return ModelStatus(\n        loaded=True,\n        model_id=MODEL_CACHE[\"model_id\"],\n        device=MODEL_CACHE[\"device\"],\n        memory_usage=memory_info,\n        optimizations=MODEL_CACHE.get(\"optimizations\", {})\n    )\n\n@app.post(\"/generate\", response_model=ImageResponse)\nasync def generate_image(request: ImageRequest, background_tasks: BackgroundTasks):\n    \"\"\"Generate image using optimized SDXL model\"\"\"\n    \n    if \"pipeline\" not in MODEL_CACHE:\n        raise HTTPException(status_code=503, detail=\"Model not loaded yet\")\n    \n    try:\n        pipeline = MODEL_CACHE[\"pipeline\"]\n        \n        # Set random seed\n        generator = torch.Generator(\"cpu\").manual_seed(request.seed) if request.seed is not None else None\n        \n        logger.info(f\"Generating: '{request.prompt[:50]}...' ({request.width}x{request.height}, {request.steps} steps)\")\n        \n        # Generate image with CPU optimization\n        with torch.inference_mode():\n            result = pipeline(\n                prompt=request.prompt,\n                negative_prompt=request.negative_prompt,\n                width=request.width,\n                height=request.height,\n                num_inference_steps=request.steps,\n                guidance_scale=request.guidance_scale,\n                generator=generator,\n                output_type=\"pil\"\n            )\n        \n        image = result.images[0]\n        \n        # Convert to base64\n        buffer = io.BytesIO()\n        image.save(buffer, format=\"PNG\", optimize=True)\n        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n        \n        # Schedule cleanup\n        background_tasks.add_task(cleanup_memory)\n        \n        # Create metadata\n        metadata = {\n            \"prompt\": request.prompt,\n            \"negative_prompt\": request.negative_prompt,\n            \"width\": request.width,\n            \"height\": request.height,\n            \"steps\": request.steps,\n            \"guidance_scale\": request.guidance_scale,\n            \"seed\": request.seed,\n            \"model\": MODEL_CACHE[\"model_id\"],\n            \"device\": \"cpu\",\n            \"optimizations\": MODEL_CACHE.get(\"optimizations\", {}),\n            \"format\": \"PNG\",\n            \"inference_type\": \"cpu-optimized\"\n        }\n        \n        logger.info(f\"✅ Generated successfully ({len(img_base64)} chars)\")\n        \n        return ImageResponse(\n            image=img_base64,\n            metadata=metadata\n        )\n        \n    except Exception as e:\n        logger.error(f\"❌ Generation failed: {e}\")\n        cleanup_memory()\n        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n\n@app.post(\"/reload-model\")\nasync def reload_model():\n    \"\"\"Reload the model (admin endpoint)\"\"\"\n    global MODEL_CACHE\n    \n    logger.info(\"Reloading model...\")\n    MODEL_CACHE.clear()\n    cleanup_memory()\n    \n    await load_optimized_model()\n    \n    return {\"status\": \"success\", \"message\": \"Model reloaded\"}\n\nif __name__ == \"__main__\":\n    port = int(os.getenv(\"PORT\", 8080))\n    logger.info(f\"Starting optimized SDXL service on port {port}\")\n    uvicorn.run(\n        app,\n        host=\"0.0.0.0\",\n        port=port,\n        access_log=True,\n        log_level=\"info\"\n    )\nPYTHON_SERVICE_EOF\necho 'Installing psutil for memory monitoring...' && pip install --no-cache-dir psutil && echo 'Starting Optimized SDXL Service...' && python /tmp/sdxl_service.py"
      ],
      "healthCheck": {
        "command": ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"],
        "interval": 60,
        "timeout": 10,
        "retries": 3,
        "startPeriod": 180
      },
      "stopTimeout": 60
    }
  ]
}
