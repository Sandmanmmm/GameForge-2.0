{
    "family": "gameforge-sdxl-optimized-task",
    "taskRoleArn": "arn:aws:iam::927588814706:role/ecsTaskRole",
    "executionRoleArn": "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
    "networkMode": "awsvpc",
    "requiresCompatibilities": ["FARGATE"],
    "cpu": "4096",
    "memory": "16384",
    "containerDefinitions": [
        {
            "name": "sdxl-optimized-service",
            "image": "public.ecr.aws/docker/library/python:3.11-slim",
            "essential": true,
            "portMappings": [
                {
                    "containerPort": 8080,
                    "protocol": "tcp",
                    "name": "sdxl-port"
                }
            ],
            "logConfiguration": {
                "logDriver": "awslogs",
                "options": {
                    "awslogs-group": "/ecs/gameforge-sdxl-optimized",
                    "awslogs-region": "us-east-1",
                    "awslogs-stream-prefix": "ecs"
                }
            },
            "environment": [
                {
                    "name": "PORT",
                    "value": "8080"
                },
                {
                    "name": "PYTHON_ENV",
                    "value": "production"
                },
                {
                    "name": "HF_HOME",
                    "value": "/tmp/huggingface_cache"
                }
            ],
            "command": [
                "bash",
                "-c",
                "apt-get update && apt-get install -y curl git build-essential && pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cpu && pip install --no-cache-dir fastapi uvicorn[standard] pillow pydantic diffusers transformers accelerate safetensors compel psutil && cat > /tmp/sdxl_optimized.py << 'PYTHON_EOF'\nimport os\nimport io\nimport base64\nimport asyncio\nimport logging\nimport gc\nfrom typing import Optional, Dict, Any\nfrom contextlib import asynccontextmanager\n\nimport torch\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel, Field\nimport uvicorn\nfrom diffusers import StableDiffusionXLPipeline, DiffusionPipeline\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nMODEL_CACHE: Dict[str, Any] = {}\nMODEL_ID = \"segmind/SSD-1B\"\n\nclass ImageRequest(BaseModel):\n    prompt: str = Field(..., min_length=1, max_length=500)\n    negative_prompt: Optional[str] = Field(None, max_length=500)\n    width: Optional[int] = Field(512, ge=256, le=1024)\n    height: Optional[int] = Field(512, ge=256, le=1024)\n    steps: Optional[int] = Field(20, ge=10, le=30)\n    guidance_scale: Optional[float] = Field(7.5, ge=1.0, le=15.0)\n    seed: Optional[int] = Field(None, ge=0, le=2147483647)\n\nclass ImageResponse(BaseModel):\n    image: str\n    metadata: Dict[str, Any]\n\nclass ModelStatus(BaseModel):\n    loaded: bool\n    model_id: str\n    device: str\n    memory_usage: Dict[str, float]\n    optimizations: Dict[str, bool]\n\ndef get_memory_info():\n    try:\n        import psutil\n        memory = psutil.virtual_memory()\n        return {\n            \"total_gb\": round(memory.total / 1024**3, 2),\n            \"available_gb\": round(memory.available / 1024**3, 2),\n            \"used_gb\": round(memory.used / 1024**3, 2),\n            \"percent\": memory.percent\n        }\n    except ImportError:\n        return {\"total_gb\": 0, \"available_gb\": 0, \"used_gb\": 0, \"percent\": 0}\n\nasync def load_optimized_model():\n    global MODEL_CACHE\n    \n    if \"pipeline\" in MODEL_CACHE:\n        logger.info(\"Model already loaded from cache\")\n        return\n    \n    logger.info(f\"Loading optimized model: {MODEL_ID}...\")\n    \n    try:\n        pipeline = DiffusionPipeline.from_pretrained(\n            MODEL_ID,\n            torch_dtype=torch.float32,\n            use_safetensors=True,\n            safety_checker=None,\n            requires_safety_checker=False\n        )\n        \n        pipeline = pipeline.to(\"cpu\")\n        \n        optimizations = {\n            \"memory_efficient_attention\": False,\n            \"cpu_offload\": True,\n            \"safety_checker_disabled\": True,\n            \"torch_compile\": False\n        }\n        \n        try:\n            pipeline.enable_attention_slicing()\n            optimizations[\"attention_slicing\"] = True\n            logger.info(\" Attention slicing enabled\")\n        except Exception as e:\n            logger.warning(f\" Attention slicing failed: {e}\")\n            optimizations[\"attention_slicing\"] = False\n        \n        try:\n            if hasattr(torch, \"compile\"):\n                pipeline.unet = torch.compile(pipeline.unet, mode=\"reduce-overhead\", fullgraph=True)\n                optimizations[\"torch_compile\"] = True\n                logger.info(\" Torch compile enabled\")\n        except Exception as e:\n            logger.warning(f\" Torch compile failed: {e}\")\n        \n        MODEL_CACHE[\"pipeline\"] = pipeline\n        MODEL_CACHE[\"model_id\"] = MODEL_ID\n        MODEL_CACHE[\"device\"] = \"cpu\"\n        MODEL_CACHE[\"optimizations\"] = optimizations\n        \n        logger.info(\" Optimized model loaded successfully\")\n        memory_info = get_memory_info()\n        logger.info(f\"Memory usage: {memory_info}\")\n        \n    except Exception as e:\n        logger.error(f\" Failed to load model: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Model loading failed: {str(e)}\")\n\ndef cleanup_memory():\n    gc.collect()\n    if torch.backends.mps.is_available():\n        torch.mps.empty_cache()\n\n@asynccontextmanager\nasync def lifespan(app: FastAPI):\n    logger.info(\" Starting GameForge Optimized SDXL Service...\")\n    await load_optimized_model()\n    yield\n    logger.info(\" Shutting down service...\")\n    cleanup_memory()\n\napp = FastAPI(\n    title=\"GameForge SDXL Optimized Service\",\n    version=\"2.1.0\",\n    description=\"CPU-optimized SDXL service with model caching\",\n    lifespan=lifespan\n)\n\n@app.get(\"/health\")\nasync def health():\n    memory_info = get_memory_info()\n    models_loaded = \"pipeline\" in MODEL_CACHE\n    \n    return {\n        \"status\": \"healthy\" if models_loaded else \"loading\",\n        \"version\": \"2.1.0\",\n        \"service\": \"sdxl-optimized\",\n        \"models_loaded\": models_loaded,\n        \"device\": \"cpu\",\n        \"memory\": memory_info\n    }\n\n@app.get(\"/model-status\", response_model=ModelStatus)\nasync def get_model_status():\n    if \"pipeline\" not in MODEL_CACHE:\n        raise HTTPException(status_code=503, detail=\"Model not loaded yet\")\n    \n    memory_info = get_memory_info()\n    \n    return ModelStatus(\n        loaded=True,\n        model_id=MODEL_CACHE[\"model_id\"],\n        device=MODEL_CACHE[\"device\"],\n        memory_usage=memory_info,\n        optimizations=MODEL_CACHE.get(\"optimizations\", {})\n    )\n\n@app.post(\"/generate\", response_model=ImageResponse)\nasync def generate_image(request: ImageRequest, background_tasks: BackgroundTasks):\n    if \"pipeline\" not in MODEL_CACHE:\n        raise HTTPException(status_code=503, detail=\"Model not loaded yet\")\n    \n    try:\n        pipeline = MODEL_CACHE[\"pipeline\"]\n        generator = torch.Generator(\"cpu\").manual_seed(request.seed) if request.seed is not None else None\n        \n        logger.info(f\"Generating: '{request.prompt[:50]}...' ({request.width}x{request.height}, {request.steps} steps)\")\n        \n        with torch.inference_mode():\n            result = pipeline(\n                prompt=request.prompt,\n                negative_prompt=request.negative_prompt,\n                width=request.width,\n                height=request.height,\n                num_inference_steps=request.steps,\n                guidance_scale=request.guidance_scale,\n                generator=generator,\n                output_type=\"pil\"\n            )\n        \n        image = result.images[0]\n        \n        buffer = io.BytesIO()\n        image.save(buffer, format=\"PNG\", optimize=True)\n        img_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n        \n        background_tasks.add_task(cleanup_memory)\n        \n        metadata = {\n            \"prompt\": request.prompt,\n            \"negative_prompt\": request.negative_prompt,\n            \"width\": request.width,\n            \"height\": request.height,\n            \"steps\": request.steps,\n            \"guidance_scale\": request.guidance_scale,\n            \"seed\": request.seed,\n            \"model\": MODEL_CACHE[\"model_id\"],\n            \"device\": \"cpu\",\n            \"optimizations\": MODEL_CACHE.get(\"optimizations\", {}),\n            \"format\": \"PNG\",\n            \"inference_type\": \"cpu-optimized\"\n        }\n        \n        logger.info(f\" Generated successfully ({len(img_base64)} chars)\")\n        \n        return ImageResponse(\n            image=img_base64,\n            metadata=metadata\n        )\n        \n    except Exception as e:\n        logger.error(f\" Generation failed: {e}\")\n        cleanup_memory()\n        raise HTTPException(status_code=500, detail=f\"Generation failed: {str(e)}\")\n\n@app.post(\"/reload-model\")\nasync def reload_model():\n    global MODEL_CACHE\n    \n    logger.info(\"Reloading model...\")\n    MODEL_CACHE.clear()\n    cleanup_memory()\n    \n    await load_optimized_model()\n    \n    return {\"status\": \"success\", \"message\": \"Model reloaded\"}\n\nif __name__ == \"__main__\":\n    port = int(os.getenv(\"PORT\", 8080))\n    logger.info(f\"Starting optimized SDXL service on port {port}\")\n    uvicorn.run(\n        app,\n        host=\"0.0.0.0\",\n        port=port,\n        access_log=True,\n        log_level=\"info\"\n    )\nPYTHON_EOF\npython /tmp/sdxl_optimized.py"
            ],
            "healthCheck": {
                "command": ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"],
                "interval": 60,
                "timeout": 10,
                "retries": 3,
                "startPeriod": 300
            },
            "stopTimeout": 60
        }
    ]
}
