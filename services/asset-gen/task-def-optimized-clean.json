{
    "family":  "gameforge-sdxl-optimized",
    "taskRoleArn":  "arn:aws:iam::927588814706:role/ecsTaskRole",
    "executionRoleArn":  "arn:aws:iam::927588814706:role/ecsTaskExecutionRole",
    "networkMode":  "awsvpc",
    "requiresCompatibilities":  [
                                    "FARGATE"
                                ],
    "cpu":  "4096",
    "memory":  "16384",
    "containerDefinitions":  [
                                 {
                                     "name":  "sdxl-optimized",
                                     "image":  "public.ecr.aws/docker/library/python:3.11-slim",
                                     "essential":  true,
                                     "portMappings":  [
                                                          {
                                                              "containerPort":  8080,
                                                              "protocol":  "tcp"
                                                          }
                                                      ],
                                     "logConfiguration":  {
                                                              "logDriver":  "awslogs",
                                                              "options":  {
                                                                              "awslogs-group":  "/ecs/gameforge-sdxl-optimized",
                                                                              "awslogs-region":  "us-east-1",
                                                                              "awslogs-stream-prefix":  "ecs"
                                                                          }
                                                          },
                                     "environment":  [
                                                         {
                                                             "name":  "PORT",
                                                             "value":  "8080"
                                                         },
                                                         {
                                                             "name":  "HF_HOME",
                                                             "value":  "/tmp/huggingface_cache"
                                                         },
                                                         {
                                                             "name":  "TRANSFORMERS_CACHE",
                                                             "value":  "/tmp/transformers_cache"
                                                         }
                                                     ],
                                     "command":  [
                                                     "bash",
                                                     "-c",
                                                     "apt-get update \u0026\u0026 apt-get install -y curl git build-essential \u0026\u0026 pip install --no-cache-dir \u0027torch\u003e=2.0.0\u0027 torchvision --index-url https://download.pytorch.org/whl/cpu \u0026\u0026 pip install --no-cache-dir fastapi \u0027uvicorn[standard]\u0027 pillow pydantic \u0027diffusers\u003e=0.21.0\u0027 \u0027transformers\u003e=4.25.0\u0027 accelerate safetensors compel psutil \u0026\u0026 python -c \"import os, io, base64, asyncio, logging, gc; from typing import Optional, Dict, Any; from contextlib import asynccontextmanager; import torch; from fastapi import FastAPI, HTTPException, BackgroundTasks; from pydantic import BaseModel, Field; import uvicorn; from diffusers import DiffusionPipeline; logging.basicConfig(level=logging.INFO); logger = logging.getLogger(__name__); MODEL_CACHE = {}; MODEL_ID = \u0027segmind/SSD-1B\u0027; class ImageRequest(BaseModel): prompt: str = Field(..., min_length=1, max_length=500); negative_prompt: Optional[str] = Field(None, max_length=500); width: Optional[int] = Field(512, ge=256, le=1024); height: Optional[int] = Field(512, ge=256, le=1024); steps: Optional[int] = Field(20, ge=10, le=30); guidance_scale: Optional[float] = Field(7.5, ge=1.0, le=15.0); seed: Optional[int] = Field(None, ge=0, le=2147483647); class ImageResponse(BaseModel): image: str; metadata: Dict[str, Any]; class ModelStatus(BaseModel): loaded: bool; model_id: str; device: str; memory_usage: Dict[str, Any]; optimizations: Dict[str, bool]; def get_memory_info(): try: import psutil; memory = psutil.virtual_memory(); return {\u0027total_gb\u0027: round(memory.total / 1024**3, 2), \u0027available_gb\u0027: round(memory.available / 1024**3, 2), \u0027used_gb\u0027: round(memory.used / 1024**3, 2), \u0027percent\u0027: memory.percent}; except ImportError: return {\u0027total_gb\u0027: 0, \u0027available_gb\u0027: 0, \u0027used_gb\u0027: 0, \u0027percent\u0027: 0}; async def load_optimized_model(): global MODEL_CACHE; if \u0027pipeline\u0027 in MODEL_CACHE: logger.info(\u0027Model already loaded from cache\u0027); return; logger.info(f\u0027Loading optimized model: {MODEL_ID}...\u0027); try: pipeline = DiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float32, use_safetensors=True, safety_checker=None, requires_safety_checker=False); pipeline = pipeline.to(\u0027cpu\u0027); optimizations = {\u0027memory_efficient_attention\u0027: False, \u0027cpu_offload\u0027: True, \u0027safety_checker_disabled\u0027: True, \u0027torch_compile\u0027: False}; try: pipeline.enable_attention_slicing(); optimizations[\u0027attention_slicing\u0027] = True; logger.info(\u0027 Attention slicing enabled\u0027); except Exception as e: logger.warning(f\u0027 Attention slicing failed: {e}\u0027); optimizations[\u0027attention_slicing\u0027] = False; MODEL_CACHE[\u0027pipeline\u0027] = pipeline; MODEL_CACHE[\u0027model_id\u0027] = MODEL_ID; MODEL_CACHE[\u0027device\u0027] = \u0027cpu\u0027; MODEL_CACHE[\u0027optimizations\u0027] = optimizations; logger.info(\u0027 Optimized model loaded successfully\u0027); memory_info = get_memory_info(); logger.info(f\u0027Memory usage: {memory_info}\u0027); except Exception as e: logger.error(f\u0027 Failed to load model: {e}\u0027); raise; def cleanup_memory(): gc.collect(); @asynccontextmanager; async def lifespan(app: FastAPI): logger.info(\u0027 Starting GameForge Optimized SDXL Service...\u0027); await load_optimized_model(); yield; logger.info(\u0027 Shutting down service...\u0027); cleanup_memory(); app = FastAPI(title=\u0027GameForge SDXL Optimized Service\u0027, version=\u00272.1.0\u0027, description=\u0027CPU-optimized SDXL service with model caching\u0027, lifespan=lifespan); @app.get(\u0027/health\u0027); async def health(): memory_info = get_memory_info(); models_loaded = \u0027pipeline\u0027 in MODEL_CACHE; return {\u0027status\u0027: \u0027healthy\u0027 if models_loaded else \u0027loading\u0027, \u0027version\u0027: \u00272.1.0\u0027, \u0027service\u0027: \u0027sdxl-optimized\u0027, \u0027models_loaded\u0027: models_loaded, \u0027device\u0027: \u0027cpu\u0027, \u0027memory\u0027: memory_info}; @app.get(\u0027/model-status\u0027, response_model=ModelStatus); async def get_model_status(): if \u0027pipeline\u0027 not in MODEL_CACHE: raise HTTPException(status_code=503, detail=\u0027Model not loaded yet\u0027); memory_info = get_memory_info(); return ModelStatus(loaded=True, model_id=MODEL_CACHE[\u0027model_id\u0027], device=MODEL_CACHE[\u0027device\u0027], memory_usage=memory_info, optimizations=MODEL_CACHE.get(\u0027optimizations\u0027, {})); @app.post(\u0027/generate\u0027, response_model=ImageResponse); async def generate_image(request: ImageRequest, background_tasks: BackgroundTasks): if \u0027pipeline\u0027 not in MODEL_CACHE: raise HTTPException(status_code=503, detail=\u0027Model not loaded yet\u0027); try: pipeline = MODEL_CACHE[\u0027pipeline\u0027]; generator = torch.Generator(\u0027cpu\u0027).manual_seed(request.seed) if request.seed is not None else None; logger.info(f\u0027Generating: {request.prompt[:50]}... ({request.width}x{request.height}, {request.steps} steps)\u0027); with torch.inference_mode(): result = pipeline(prompt=request.prompt, negative_prompt=request.negative_prompt, width=request.width, height=request.height, num_inference_steps=request.steps, guidance_scale=request.guidance_scale, generator=generator, output_type=\u0027pil\u0027); image = result.images[0]; buffer = io.BytesIO(); image.save(buffer, format=\u0027PNG\u0027, optimize=True); img_base64 = base64.b64encode(buffer.getvalue()).decode(\u0027utf-8\u0027); background_tasks.add_task(cleanup_memory); metadata = {\u0027prompt\u0027: request.prompt, \u0027negative_prompt\u0027: request.negative_prompt, \u0027width\u0027: request.width, \u0027height\u0027: request.height, \u0027steps\u0027: request.steps, \u0027guidance_scale\u0027: request.guidance_scale, \u0027seed\u0027: request.seed, \u0027model\u0027: MODEL_CACHE[\u0027model_id\u0027], \u0027device\u0027: \u0027cpu\u0027, \u0027optimizations\u0027: MODEL_CACHE.get(\u0027optimizations\u0027, {}), \u0027format\u0027: \u0027PNG\u0027, \u0027inference_type\u0027: \u0027cpu-optimized\u0027}; logger.info(f\u0027 Generated successfully ({len(img_base64)} chars)\u0027); return ImageResponse(image=img_base64, metadata=metadata); except Exception as e: logger.error(f\u0027 Generation failed: {e}\u0027); cleanup_memory(); raise HTTPException(status_code=500, detail=f\u0027Generation failed: {str(e)}\u0027); @app.post(\u0027/reload-model\u0027); async def reload_model(): global MODEL_CACHE; logger.info(\u0027Reloading model...\u0027); MODEL_CACHE.clear(); cleanup_memory(); await load_optimized_model(); return {\u0027status\u0027: \u0027success\u0027, \u0027message\u0027: \u0027Model reloaded\u0027}; port = int(os.getenv(\u0027PORT\u0027, 8080)); logger.info(f\u0027Starting optimized SDXL service on port {port}\u0027); uvicorn.run(app, host=\u00270.0.0.0\u0027, port=port, access_log=True, log_level=\u0027info\u0027)\""
                                                 ],
                                     "healthCheck":  {
                                                         "command":  [
                                                                         "CMD-SHELL",
                                                                         "curl -f http://localhost:8080/health || exit 1"
                                                                     ],
                                                         "interval":  60,
                                                         "timeout":  10,
                                                         "retries":  3,
                                                         "startPeriod":  300
                                                     },
                                     "stopTimeout":  60
                                 }
                             ]
}
