# GameForge Production Stack - Maximum Security Hardening
# ========================================================================
# Enterprise production deployment with comprehensive security hardening:
# - Seccomp profiles for syscall filtering
# - AppArmor profiles for mandatory access control
# - Dropped capabilities and security contexts
# - Network isolation and resource limits
# - Read-only filesystems and tmpfs mounts
# ========================================================================

version: '3.8'

# ========================================================================
# Security Context Templates
# ========================================================================
x-app-security: &app-security
  user: "1001:1001"
  read_only: false  # Phase 4: Allow model cache and temporary files
  tmpfs:
    - /tmp:size=2G,mode=1777  # Phase 4: Increased for model storage
    - /app/tmp:noexec,nosuid,size=500m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/gameforge-app.json
    - apparmor:gameforge-app
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - FOWNER
    - DAC_OVERRIDE

# Phase 4: Vault Security Template
x-vault-security: &vault-security
  user: "100:1000"
  read_only: true
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/vault.json
  cap_drop:
    - ALL
  cap_add:
    - IPC_LOCK
    - SETUID
    - SETGID

x-web-security: &web-security
  user: "101:101"
  read_only: true
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
    - /var/cache/nginx:noexec,nosuid,size=500m
    - /var/run:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/nginx.json
    - apparmor:nginx-container
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - NET_BIND_SERVICE
    - DAC_OVERRIDE

x-db-security: &db-security
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
    - /var/run:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=./security/seccomp/database.json
    - apparmor:database-container
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - FOWNER
    - DAC_OVERRIDE

# ========================================================================
# Resource Limit Templates
# ========================================================================
x-app-resources: &app-resources
  deploy:
    resources:
      limits:
        memory: 8G
        cpus: '4.0'
        pids: 1000
      reservations:
        memory: 2G
        cpus: '1.0'
    restart_policy:
      condition: on-failure
      delay: 30s
      max_attempts: 3

# Phase 4: Vault Resource Template
x-vault-resources: &vault-resources
  deploy:
    resources:
      limits:
        memory: 1G
        cpus: '1.0'
        pids: 100
      reservations:
        memory: 256M
        cpus: '0.25'
    restart_policy:
      condition: on-failure
      delay: 15s
      max_attempts: 5

x-db-resources: &db-resources
  deploy:
    resources:
      limits:
        memory: 4G
        cpus: '2.0'
        pids: 500
      reservations:
        memory: 1G
        cpus: '0.5'

x-web-resources: &web-resources
  deploy:
    resources:
      limits:
        memory: 1G
        cpus: '1.0'
        pids: 200
      reservations:
        memory: 256M
        cpus: '0.25'

# ========================================================================
# Common Configuration
# ========================================================================
x-common-env: &common-env
  GAMEFORGE_ENV: production
  LOG_LEVEL: info
  TZ: UTC
  PYTHONPATH: /app

x-secure-logging: &secure-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"
    labels: "service,environment,security"

services:
  # ========================================================================
  # GameForge Application Service (Phase 4: Maximum Security + Model Asset Management)
  # ========================================================================
  gameforge-app:
    build:
      context: .
      dockerfile: Dockerfile.production.enhanced
      target: production
      args:
        BUILD_ENV: phase4-production
        ENABLE_GPU: "true"
    image: gameforge:phase4-production-secure
    container_name: gameforge-app-phase4-secure
    hostname: gameforge-app
    restart: unless-stopped
    <<: *app-security
    # GPU optimization configuration
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 12G  # Reduced from 16G for better resource efficiency
          cpus: '6.0'  # Reduced from 8.0 for better utilization
          pids: 1000
        reservations:
          memory: 4G
          cpus: '2.0'
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
    environment:
      <<: *common-env
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/0
      ELASTICSEARCH_URL: http://elasticsearch:9200
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      SECRET_KEY: ${SECRET_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      # Phase 4: Model Security Configuration
      MODEL_SECURITY_ENABLED: "true"
      SECURITY_SCAN_ENABLED: "true"
      STRICT_MODEL_SECURITY: "true"
      VAULT_HEALTH_CHECK_ENABLED: "true"
      PERFORMANCE_MONITORING_ENABLED: "true"
      # Phase 4: Vault Integration
      VAULT_ADDR: "http://vault:8200"
      VAULT_TOKEN: ${VAULT_TOKEN}
      VAULT_NAMESPACE: "gameforge"
      # Phase 4: Model Storage Configuration
      MODEL_STORAGE_BACKEND: "s3"
      AWS_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_S3_BUCKET: ${MODEL_S3_BUCKET}
      MODEL_CACHE_DIR: "/tmp/models"
      REQUIRED_MODELS: ${REQUIRED_MODELS:-""}
      # GPU optimization environment
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:512,garbage_collection_threshold:0.6,expandable_segments:True
      CUDA_LAUNCH_BLOCKING: 0
      CUDA_CACHE_DISABLE: 0
      PYTORCH_JIT: 1
      PYTORCH_JIT_LOG_LEVEL: "ERROR"
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      # Phase 4: Enhanced script mounting and model cache
      - ./scripts:/app/scripts:ro
      - model-cache:/tmp/models:rw
      - monitoring-data:/tmp/monitoring:rw
    networks:
      - frontend
      - backend
      - monitoring
      - vault-network  # Phase 4: Vault network access
    ports:
      - "127.0.0.1:8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=phase4-production"
      - "security.compliance=required"
      - "ai.gpu.enabled=true"
      - "ai.gpu.driver=nvidia"
      - "phase4.model-security=enabled"
      - "phase4.vault-integration=enabled"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      vault:
        condition: service_healthy
    # Phase 4: Enhanced entrypoint
    entrypoint: ["/app/scripts/entrypoint-phase4.sh"]
    command: ["python", "-m", "gameforge.main"]

  # ========================================================================
  # Nginx Reverse Proxy (Maximum Security)
  # ========================================================================
  nginx:
    image: nginx:1.24.0-alpine
    container_name: gameforge-nginx-secure
    hostname: nginx
    restart: unless-stopped
    <<: [*web-security, *web-resources]
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl/certs:/etc/ssl/certs:ro
      - ./ssl/private:/etc/ssl/private:ro
      - nginx-logs:/var/log/nginx:rw
      - static-files:/var/www/html:ro
    networks:
      - frontend
    ports:
      - "80:80"
      - "443:443"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=web-server"
    depends_on:
      gameforge-app:
        condition: service_healthy

  # ========================================================================
  # PostgreSQL Database (Hardened)
  # ========================================================================
  postgres:
    image: postgres:15.4-alpine
    container_name: gameforge-postgres-secure
    hostname: postgres
    restart: unless-stopped
    <<: [*db-security, *db-resources]
    user: "999:999"
    environment:
      POSTGRES_DB: gameforge_prod
      POSTGRES_USER: gameforge
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    volumes:
      - postgres-data:/var/lib/postgresql/data:rw
      - ./database_setup.sql:/docker-entrypoint-initdb.d/01-setup.sql:ro
      - postgres-logs:/var/log/postgresql:rw
    tmpfs:
      - /var/lib/postgresql/data/pg_stat_tmp:noexec,nosuid,size=100m
    networks:
      - backend
    ports:
      - "127.0.0.1:5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U gameforge -d gameforge_prod"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=database"
    command: >
      postgres
      -c ssl=on
      -c ssl_cert_file=/var/lib/postgresql/server.crt
      -c ssl_key_file=/var/lib/postgresql/server.key
      -c shared_preload_libraries=pg_stat_statements
      -c max_connections=200
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
      -c log_statement=mod
      -c log_min_duration_statement=1000

  # ========================================================================
  # Redis Cache (Hardened)
  # ========================================================================
  redis:
    image: redis:7.2.1-alpine
    container_name: gameforge-redis-secure
    hostname: redis
    restart: unless-stopped
    <<: *db-security
    user: "999:999"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data:rw
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - backend
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=cache"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru

  # ========================================================================
  # HashiCorp Vault (Phase 4: Model Asset Security)
  # ========================================================================
  vault:
    image: hashicorp/vault:latest
    container_name: gameforge-vault-secure
    hostname: vault
    restart: unless-stopped
    <<: [*vault-security, *vault-resources]
    environment:
      VAULT_ADDR: "http://0.0.0.0:8200"
      VAULT_API_ADDR: "http://vault:8200"
      VAULT_CLUSTER_ADDR: "http://vault:8201"
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_ROOT_TOKEN}
      VAULT_DEV_LISTEN_ADDRESS: "0.0.0.0:8200"
      VAULT_LOCAL_CONFIG: |
        {
          "backend": {
            "file": {
              "path": "/vault/data"
            }
          },
          "listener": {
            "tcp": {
              "address": "0.0.0.0:8200",
              "tls_disable": true
            }
          },
          "default_lease_ttl": "168h",
          "max_lease_ttl": "720h",
          "ui": true,
          "log_level": "INFO"
        }
    volumes:
      - vault-data:/vault/data:rw
      - vault-logs:/vault/logs:rw
      - ./vault/config:/vault/config:ro
      - ./vault/policies:/vault/policies:ro
    networks:
      - backend
      - vault-network
    ports:
      - "127.0.0.1:8200:8200"
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=vault"
      - "phase4.vault=enabled"
      - "phase4.model-security=enabled"
    command: >
      vault server
      -config=/vault/config
      -dev-root-token-id=${VAULT_ROOT_TOKEN}
      -dev-listen-address=0.0.0.0:8200

  # ========================================================================
  # Elasticsearch (Hardened)
  # ========================================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.2
    container_name: gameforge-elasticsearch-secure
    hostname: elasticsearch
    restart: unless-stopped
    user: "1000:1000"
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    environment:
      - node.name=gameforge-elasticsearch
      - cluster.name=gameforge-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - xpack.license.self_generated.type=basic
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data:rw
      - elasticsearch-logs:/usr/share/elasticsearch/logs:rw
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    networks:
      - backend
      - monitoring
    ports:
      - "127.0.0.1:9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -u elastic:${ELASTIC_PASSWORD} -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=search-engine"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '0.5'

  # ========================================================================
  # Logstash Log Processing (Hardened)
  # ========================================================================
  logstash:
    image: docker.elastic.co/logstash/logstash:8.9.2
    container_name: gameforge-logstash-secure
    hostname: logstash
    restart: unless-stopped
    user: "1000:1000"
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    environment:
      - "LS_JAVA_OPTS=-Xmx1g -Xms1g"
      - LOGSTASH_SYSTEM_PASSWORD=${LOGSTASH_SYSTEM_PASSWORD}
      - xpack.monitoring.enabled=true
      - xpack.monitoring.elasticsearch.hosts=["elasticsearch:9200"]
      - xpack.monitoring.elasticsearch.username=logstash_system
      - xpack.monitoring.elasticsearch.password=${LOGSTASH_SYSTEM_PASSWORD}
    volumes:
      - ./monitoring/logging/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - logstash-data:/usr/share/logstash/data:rw
    networks:
      - backend
      - monitoring
    ports:
      - "127.0.0.1:5044:5044"  # Beats input
      - "127.0.0.1:8080:8080"  # HTTP input
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9600/_node/stats"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=log-processor"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.2'
    depends_on:
      elasticsearch:
        condition: service_healthy

  # ========================================================================
  # Filebeat Log Collection (Hardened)
  # ========================================================================
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.9.2
    container_name: gameforge-filebeat-secure
    hostname: filebeat
    restart: unless-stopped
    user: "root"  # Required for log file access
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /usr/share/filebeat/data:noexec,nosuid,size=500m
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
    command: [
      "--strict.perms=false",
      "-e",
      "-E", "output.logstash.hosts=[\"logstash:5044\"]",
      "-E", "setup.kibana.host=grafana:3000"
    ]
    environment:
      - FILEBEAT_SYSTEM_PASSWORD=${FILEBEAT_SYSTEM_PASSWORD}
      - NODE_NAME=gameforge-filebeat
    volumes:
      - ./monitoring/logging/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - gameforge-logs:/var/log/gameforge:ro
      - nginx-logs:/var/log/nginx:ro
      - postgres-logs:/var/log/postgresql:ro
      - elasticsearch-logs:/var/log/elasticsearch:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - backend
      - monitoring
    healthcheck:
      test: ["CMD", "filebeat", "test", "config"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=log-collector"
      - "co.elastic.logs/enabled=false"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    depends_on:
      logstash:
        condition: service_healthy

  # ========================================================================
  # Background Workers (Hardened)
  # ========================================================================
  gameforge-worker:
    build:
      context: .
      dockerfile: Dockerfile.production
      target: production
    image: gameforge:production-secure
    container_name: gameforge-worker-secure
    hostname: gameforge-worker
    restart: unless-stopped
    <<: *app-security
    # GPU optimization for AI worker tasks
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
          pids: 500
        reservations:
          memory: 1G
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
    environment:
      <<: *common-env
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/0
      ELASTICSEARCH_URL: http://elasticsearch:9200
      WORKER_TYPE: background
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/2
      # GPU optimization environment for workers
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:256,garbage_collection_threshold:0.6,expandable_segments:True
      CUDA_LAUNCH_BLOCKING: 0
      CUDA_CACHE_DISABLE: 0
      PYTORCH_JIT: 1
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      - gameforge-models:/app/models_cache:rw
    networks:
      - backend
      - monitoring
    healthcheck:
      test: ["CMD", "python", "-c", "import celery; print('Worker healthy')"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=worker"
      - "ai.gpu.enabled=true"
      - "ai.gpu.driver=nvidia"
    command: ["celery", "worker", "-A", "gameforge.celery", "--loglevel=info", "--concurrency=4"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  # ========================================================================
  # Backup Service (Hardened)
  # ========================================================================
  backup-service:
    image: postgres:15.4-alpine
    container_name: gameforge-backup-secure
    hostname: backup-service
    restart: unless-stopped
    user: "1001:1001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
      - /var/run:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    environment:
      POSTGRES_DB: gameforge_prod
      POSTGRES_USER: gameforge
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      BACKUP_SCHEDULE: "0 2 * * *"  # Daily at 2 AM
      BACKUP_RETENTION_DAYS: 30
      S3_BUCKET: ${BACKUP_S3_BUCKET}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - backup-data:/backups:rw
      - ./scripts/backup.sh:/usr/local/bin/backup.sh:ro
      - ./scripts/restore.sh:/usr/local/bin/restore.sh:ro
    networks:
      - backend
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=backup"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: ["sh", "-c", "crond -f -d 8"]
    depends_on:
      postgres:
        condition: service_healthy

  # ========================================================================
  # Prometheus Monitoring (Hardened)
  # ========================================================================
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: gameforge-prometheus-secure
    hostname: prometheus
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus:rw
    networks:
      - monitoring
    ports:
      - "127.0.0.1:9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=monitoring"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.2'

  # ========================================================================
  # Grafana Dashboard (Hardened)
  # ========================================================================
  grafana:
    image: grafana/grafana:10.1.2
    container_name: gameforge-grafana-secure
    hostname: grafana
    restart: unless-stopped
    user: "472:472"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/lib/grafana/plugins:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=./security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SECURITY_DISABLE_GRAVATAR: true
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
      GF_SECURITY_COOKIE_SECURE: true
      GF_SECURITY_COOKIE_SAMESITE: strict
      GF_SERVER_ROOT_URL: https://${DOMAIN}/grafana/
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana:rw
    networks:
      - monitoring
      - frontend
    ports:
      - "127.0.0.1:3000:3000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=dashboard"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    depends_on:
      prometheus:
        condition: service_healthy

# ========================================================================
# Network Configuration (Isolated)
# ========================================================================
networks:
  frontend:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "false"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1

  backend:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1

  monitoring:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1

  # Phase 4: Vault Network for Model Security
  vault-network:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.23.0.0/24
          gateway: 172.23.0.1

# ========================================================================
# Volume Configuration (Secure)
# ========================================================================
volumes:
  gameforge-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/logs
      o: bind,noexec,nosuid

  gameforge-cache:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/cache
      o: bind,noexec,nosuid

  gameforge-assets:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/assets
      o: bind,noexec,nosuid

  gameforge-models:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/models
      o: bind,noexec,nosuid

  # Phase 4: Model Cache Volume (Temporary, secure)
  model-cache:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=4G,mode=1777,nodev,nosuid

  # Phase 4: Monitoring Data Volume
  monitoring-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/monitoring
      o: bind,noexec,nosuid

  # Phase 4: Vault Data Volume
  vault-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/vault/data
      o: bind,nodev,nosuid

  # Phase 4: Vault Logs Volume
  vault-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/vault/logs
      o: bind,noexec,nosuid

  postgres-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/postgres
      o: bind,nodev,nosuid

  postgres-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/postgres-logs
      o: bind,noexec,nosuid

  redis-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/redis
      o: bind,nodev,nosuid

  elasticsearch-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/elasticsearch
      o: bind,nodev,nosuid

  elasticsearch-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/elasticsearch-logs
      o: bind,noexec,nosuid

  logstash-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/logstash
      o: bind,nodev,nosuid

  nginx-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/nginx-logs
      o: bind,noexec,nosuid

  static-files:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/static
      o: bind,noexec,nosuid,ro

  backup-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/backups
      o: bind,nodev,nosuid

  prometheus-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/prometheus
      o: bind,nodev,nosuid

  grafana-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/grafana
      o: bind,nodev,nosuid
