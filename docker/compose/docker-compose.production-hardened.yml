# GameForge Production Stack - Maximum Security Hardening
# ========================================================================
# Enterprise production deployment with comprehensive security hardening:
# - Seccomp profiles for syscall filtering
# - AppArmor profiles for mandatory access control
# - Dropped capabilities and security contexts
# - Network isolation and resource limits
# - Read-only filesystems and tmpfs mounts
# ========================================================================

version: '3.8'

# ========================================================================
# Security Context Templates
# ========================================================================
x-app-security: &app-security
  user: "1001:1001"
  read_only: false  # Phase 4: Allow model cache and temporary files
  tmpfs:
    - /tmp:size=2G,mode=1777  # Phase 4: Increased for model storage
    - /app/tmp:noexec,nosuid,size=500m
  security_opt:
    - no-new-privileges:true
    - seccomp=../../security/seccomp/gameforge-app.json
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - FOWNER
    - DAC_OVERRIDE

# Phase 4: Vault Security Template
x-vault-security: &vault-security
  user: "100:1000"
  read_only: true
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=../../security/seccomp/vault.json
  cap_drop:
    - ALL
  cap_add:
    - IPC_LOCK
    - SETUID
    - SETGID

x-web-security: &web-security
  user: "101:101"
  read_only: true
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
    - /var/cache/nginx:noexec,nosuid,size=500m
    - /var/run:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=../../security/seccomp/nginx.json
    - apparmor:nginx-container
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - NET_BIND_SERVICE
    - DAC_OVERRIDE

x-db-security: &db-security
  tmpfs:
    - /tmp:noexec,nosuid,size=100m
    - /var/run:noexec,nosuid,size=100m
  security_opt:
    - no-new-privileges:true
    - seccomp=../../security/seccomp/database.json
    - apparmor:database-container
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - FOWNER
    - DAC_OVERRIDE

# ========================================================================
# Resource Limit Templates
# ========================================================================
x-app-resources: &app-resources
  deploy:
    resources:
      limits:
        memory: 8G
        cpus: '4.0'
        pids: 1000
      reservations:
        memory: 2G
        cpus: '1.0'
    restart_policy:
      condition: on-failure
      delay: 30s
      max_attempts: 3

# Phase 4: Vault Resource Template
x-vault-resources: &vault-resources
  deploy:
    resources:
      limits:
        memory: 1G
        cpus: '1.0'
        pids: 100
      reservations:
        memory: 256M
        cpus: '0.25'
    restart_policy:
      condition: on-failure
      delay: 15s
      max_attempts: 5

# GPU Resource Templates
x-gpu-inference-resources: &gpu-inference-resources
  deploy:
    resources:
      limits:
        memory: 16G
        cpus: '8.0'
        pids: 2000
      reservations:
        memory: 8G
        cpus: '4.0'
    restart_policy:
      condition: on-failure
      delay: 30s
      max_attempts: 3

x-gpu-training-resources: &gpu-training-resources
  deploy:
    resources:
      limits:
        memory: 32G
        cpus: '16.0'
        pids: 4000
      reservations:
        memory: 16G
        cpus: '8.0'
    restart_policy:
      condition: on-failure
      delay: 60s
      max_attempts: 2

x-db-resources: &db-resources
  deploy:
    resources:
      limits:
        memory: 4G
        cpus: '2.0'
        pids: 500
      reservations:
        memory: 1G
        cpus: '0.5'

x-web-resources: &web-resources
  deploy:
    resources:
      limits:
        memory: 1G
        cpus: '1.0'
        pids: 200
      reservations:
        memory: 256M
        cpus: '0.25'

# ========================================================================
# ML Platform Security & Resource Templates
# ========================================================================
x-ml-security: &ml-security
  user: "1002:1002"
  read_only: false  # MLflow needs write access for tracking
  tmpfs:
    - /tmp:size=1G,mode=1777
    - /app/tmp:noexec,nosuid,size=500m
  security_opt:
    - no-new-privileges:true
    - seccomp=../../security/seccomp/gameforge-app.json
  cap_drop:
    - ALL
  cap_add:
    - CHOWN
    - SETUID
    - SETGID
    - FOWNER
    - DAC_OVERRIDE

x-ml-resources: &ml-resources
  deploy:
    resources:
      limits:
        memory: 4G
        cpus: '2.0'
        pids: 500
      reservations:
        memory: 1G
        cpus: '0.5'
    restart_policy:
      condition: on-failure
      delay: 30s
      max_attempts: 3

x-ml-db-resources: &ml-db-resources
  deploy:
    resources:
      limits:
        memory: 2G
        cpus: '1.0'
        pids: 200
      reservations:
        memory: 512M
        cpus: '0.25'
    restart_policy:
      condition: on-failure
      delay: 30s
      max_attempts: 3

# ========================================================================
# Common Configuration
# ========================================================================
x-common-env: &common-env
  GAMEFORGE_ENV: production
  LOG_LEVEL: info
  TZ: UTC
  PYTHONPATH: /app

x-secure-logging: &secure-logging
  driver: "json-file"
  options:
    max-size: "10m"
    max-file: "3"
    compress: "true"
    labels: "service,environment,security"

services:
  # ========================================================================
  # Security Bootstrap Service (One-shot privileged initialization)
  # ========================================================================
  security-bootstrap:
    image: gameforge-security-init:latest
    container_name: gameforge-security-bootstrap
    restart: "no"  # One-shot execution only
    privileged: true
    volumes:
      - security-shared:/shared
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
    environment:
      - SECURITY_MODE=bootstrap
      - LOG_LEVEL=info
      - SECURITY_GATE_THRESHOLD=75
      - BOOTSTRAP_ONLY=true
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
        labels: "service=security-bootstrap,environment=production,security=critical"
    labels:
      - "traefik.enable=false"
      - "security.role=bootstrap"
      - "security.criticality=high"
      - "security.execution=one-shot"
    # No healthcheck for one-shot bootstrap
    entrypoint: ["/usr/local/bin/security-bootstrap.sh"]

  # ========================================================================
  # Security Health Monitor (Non-privileged periodic monitoring)
  # ========================================================================
  security-monitor:
    image: gameforge-security-init:latest
    container_name: gameforge-security-monitor
    restart: unless-stopped
    privileged: false  # No privileges needed for monitoring
    user: "1001:1001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/monitor:nosuid,size=50m  # Writable space for monitor data (removed noexec)
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json  # Fixed with arch_prctl and rseq
    cap_drop:
      - ALL
    volumes:
      - security-shared:/shared:ro  # Read-only access to security data
    environment:
      - SECURITY_MODE=monitor
      - LOG_LEVEL=info
      - MONITOR_INTERVAL=300  # 5 minutes
    networks:
      - backend
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
        compress: "true"
        labels: "service=security-monitor,environment=production,security=monitoring"
    labels:
      - "traefik.enable=false"
      - "security.role=monitoring"
      - "security.criticality=medium"
    healthcheck:
      test: ["CMD", "/usr/local/bin/monitor-health-check.sh"]
      interval: 60s
      timeout: 10s
      start_period: 30s
      retries: 3
    entrypoint: ["/usr/local/bin/security-monitor.sh", "loop"]
    depends_on:
      security-bootstrap:
        condition: service_completed_successfully

  # ========================================================================
  # GameForge Application Service (Phase 2: Enhanced Multi-stage Build + Phase 4: Model Asset Security)
  # ========================================================================
  gameforge-app:
    build:
      context: .
      dockerfile: Dockerfile.production
      target: production
      args:
        # Phase 2: Enhanced Multi-stage Build Arguments
        BUILD_DATE: ${BUILD_DATE:-}
        VCS_REF: ${VCS_REF:-}
        BUILD_VERSION: ${BUILD_VERSION:-latest}
        VARIANT: ${GAMEFORGE_VARIANT:-gpu}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.10}
        DEBIAN_FRONTEND: noninteractive
        # Phase 2: Dynamic base image selection
        GPU_BASE_IMAGE: ${GPU_BASE_IMAGE:-nvidia/cuda:12.1-runtime-alpine}
        CPU_BASE_IMAGE: ${CPU_BASE_IMAGE:-python:3.10-alpine}
        # Phase 2: Build optimization flags
        BUILD_ENV: phase2-phase4-production
        ENABLE_GPU: ${ENABLE_GPU:-true}
        COMPILE_BYTECODE: ${COMPILE_BYTECODE:-true}
        SECURITY_HARDENING: ${SECURITY_HARDENING:-true}
    image: gameforge:phase2-phase4-production-${GAMEFORGE_VARIANT:-gpu}
    container_name: gameforge-app-phase2-phase4-secure
    hostname: gameforge-app
    restart: unless-stopped
    <<: *app-security
    # Phase 2: Dynamic GPU/CPU runtime configuration
    # runtime: ${DOCKER_RUNTIME:-nvidia}  # Commented out for CPU deployment
    deploy:
      resources:
        limits:
          # Phase 2: Dynamic resource limits based on variant
          memory: ${MEMORY_LIMIT:-12G}  # Configurable memory limit
          cpus: ${CPU_LIMIT:-6.0}      # Configurable CPU limit
          pids: 1000
        reservations:
          memory: ${MEMORY_RESERVATION:-4G}
          cpus: ${CPU_RESERVATION:-2.0}
          # Phase 2: GPU reservations for GPU variant (disabled for CPU deployment)
          # devices:
          #   - driver: nvidia
          #     count: ${GPU_COUNT:-1}
          #     capabilities: [gpu]
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
    environment:
      <<: *common-env
      # Phase 2: Variant-specific environment
      GAMEFORGE_VARIANT: ${GAMEFORGE_VARIANT:-gpu}
      BUILD_VERSION: ${BUILD_VERSION:-latest}
      VCS_REF: ${VCS_REF:-}
      # Database connections
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/0
      ELASTICSEARCH_URL: http://elasticsearch:9200
      # Security keys
      JWT_SECRET_KEY: ${JWT_SECRET_KEY}
      SECRET_KEY: ${SECRET_KEY}
      OPENAI_API_KEY: ${OPENAI_API_KEY}
      # Phase 4: Model Security Configuration
      MODEL_SECURITY_ENABLED: "true"
      SECURITY_SCAN_ENABLED: "true"
      STRICT_MODEL_SECURITY: "true"
      VAULT_HEALTH_CHECK_ENABLED: "true"
      PERFORMANCE_MONITORING_ENABLED: "true"
      # Phase 4: Vault Integration
      VAULT_ADDR: "http://vault:8200"
      VAULT_TOKEN: ${VAULT_TOKEN}
      VAULT_NAMESPACE: "gameforge"
      # Phase 4: Model Storage Configuration
      MODEL_STORAGE_BACKEND: "s3"
      AWS_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_S3_BUCKET: ${MODEL_S3_BUCKET}
      MODEL_CACHE_DIR: "/tmp/models"
      REQUIRED_MODELS: ${REQUIRED_MODELS:-""}
      # OpenTelemetry Configuration
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
      OTEL_EXPORTER_OTLP_PROTOCOL: "grpc"
      OTEL_SERVICE_NAME: "gameforge-app"
      OTEL_SERVICE_VERSION: ${BUILD_VERSION:-latest}
      OTEL_RESOURCE_ATTRIBUTES: "service.namespace=gameforge,deployment.environment=production"
      OTEL_TRACES_EXPORTER: "otlp"
      OTEL_METRICS_EXPORTER: "otlp"
      OTEL_LOGS_EXPORTER: "otlp"
      OTEL_PROPAGATORS: "tracecontext,baggage"
      OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_REQUEST: "content-type,accept,user-agent,x-forwarded-for"
      OTEL_INSTRUMENTATION_HTTP_CAPTURE_HEADERS_SERVER_RESPONSE: "content-type,content-length"
      # Phase 2: GPU optimization environment (when GPU variant)
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVIDIA_DRIVER_CAPABILITIES: ${NVIDIA_DRIVER_CAPABILITIES:-compute,utility}
      PYTORCH_CUDA_ALLOC_CONF: ${PYTORCH_CUDA_ALLOC_CONF:-max_split_size_mb:512,garbage_collection_threshold:0.6,expandable_segments:True}
      CUDA_LAUNCH_BLOCKING: ${CUDA_LAUNCH_BLOCKING:-0}
      CUDA_CACHE_DISABLE: ${CUDA_CACHE_DISABLE:-0}
      PYTORCH_JIT: ${PYTORCH_JIT:-1}
      PYTORCH_JIT_LOG_LEVEL: ${PYTORCH_JIT_LOG_LEVEL:-ERROR}
      # GPU Workload Coordination
      GPU_INFERENCE_ENDPOINT: "http://gameforge-gpu-inference:8080"
      GPU_TRAINING_ENDPOINT: "http://gameforge-gpu-training:8080"
      GPU_WORKLOAD_BALANCER: "round-robin"
      GPU_MEMORY_MONITORING: "true"
      # Phase 2: Performance tuning
      WORKERS: ${WORKERS:-4}
      MAX_WORKERS: ${MAX_WORKERS:-8}
      WORKER_TIMEOUT: ${WORKER_TIMEOUT:-300}
      WORKER_CLASS: ${WORKER_CLASS:-uvicorn.workers.UvicornWorker}
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      # Phase 4: Enhanced script mounting and model cache
      - ./scripts:/app/scripts:ro
      - model-cache:/tmp/models:rw
      - monitoring-data:/tmp/monitoring:rw
    networks:
      - frontend
      - backend
      - monitoring
      - vault-network  # Phase 4: Vault network access
      - gpu-network    # GPU workload coordination
    ports:
      - "127.0.0.1:8080:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=phase2-phase4-production"
      - "security.compliance=required"
      - "gameforge.phase=2+4"
      - "gameforge.variant=${GAMEFORGE_VARIANT:-gpu}"
      - "phase2.multi-stage=enabled"
      - "phase2.cpu-gpu-variants=enabled"
      - "phase2.build-optimization=enabled"
      - "phase4.model-security=enabled"
      - "phase4.vault-integration=enabled"
      - "ai.gpu.enabled=${ENABLE_GPU:-true}"
      - "ai.gpu.driver=${DOCKER_RUNTIME:-nvidia}"
    depends_on:
      security-bootstrap:
        condition: service_completed_successfully
      security-monitor:
        condition: service_healthy
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      vault:
        condition: service_healthy
      otel-collector:
        condition: service_healthy
    # Phase 4: Enhanced entrypoint
    entrypoint: ["/app/scripts/entrypoint-phase4.sh"]
    command: ["python", "-m", "gameforge.main"]

  # ========================================================================
  # Nginx Reverse Proxy (Maximum Security)
  # ========================================================================
  nginx:
    image: nginx:1.24.0-alpine
    container_name: gameforge-nginx-secure
    hostname: nginx
    restart: unless-stopped
    <<: [*web-security, *web-resources]
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/conf.d:/etc/nginx/conf.d:ro
      - ./ssl/certs:/etc/ssl/certs:ro
      - ./ssl/private:/etc/ssl/private:ro
      - nginx-logs:/var/log/nginx:rw
      - static-files:/var/www/html:ro
    networks:
      - frontend
    ports:
      - "80:80"
      - "443:443"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=web-server"
    depends_on:
      security-monitor:
        condition: service_healthy
      gameforge-app:
        condition: service_healthy

  # ========================================================================
  # PostgreSQL Database (Hardened)
  # ========================================================================
  postgres:
    image: postgres:15.4-alpine
    container_name: gameforge-postgres-secure
    hostname: postgres
    restart: unless-stopped
    <<: [*db-security, *db-resources]
    user: "999:999"
    environment:
      POSTGRES_DB: gameforge_prod
      POSTGRES_USER: gameforge
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    volumes:
      - postgres-data:/var/lib/postgresql/data:rw
      - ./database_setup.sql:/docker-entrypoint-initdb.d/01-setup.sql:ro
      - postgres-logs:/var/log/postgresql:rw
    tmpfs:
      - /var/lib/postgresql/data/pg_stat_tmp:noexec,nosuid,size=100m
    networks:
      - backend
    ports:
      - "127.0.0.1:5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U gameforge -d gameforge_prod"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=database"
    command: >
      postgres
      -c ssl=on
      -c ssl_cert_file=/var/lib/postgresql/server.crt
      -c ssl_key_file=/var/lib/postgresql/server.key
      -c shared_preload_libraries=pg_stat_statements
      -c max_connections=200
      -c shared_buffers=1GB
      -c effective_cache_size=3GB
      -c log_statement=mod
      -c log_min_duration_statement=1000

  # ========================================================================
  # Redis Cache (Hardened)
  # ========================================================================
  redis:
    image: redis:7.2.1-alpine
    container_name: gameforge-redis-secure
    hostname: redis
    restart: unless-stopped
    <<: *db-security
    user: "999:999"
    environment:
      REDIS_PASSWORD: ${REDIS_PASSWORD}
    volumes:
      - redis-data:/data:rw
      - ./redis/redis.conf:/usr/local/etc/redis/redis.conf:ro
    networks:
      - backend
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=cache"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: >
      redis-server /usr/local/etc/redis/redis.conf
      --requirepass ${REDIS_PASSWORD}
      --maxmemory 1gb
      --maxmemory-policy allkeys-lru

  # ========================================================================
  # HashiCorp Vault (Phase 4: Model Asset Security)
  # ========================================================================
  vault:
    image: hashicorp/vault:latest
    container_name: gameforge-vault-secure
    hostname: vault
    restart: unless-stopped
    <<: [*vault-security, *vault-resources]
    environment:
      VAULT_ADDR: "http://0.0.0.0:8200"
      VAULT_API_ADDR: "http://vault:8200"
      VAULT_CLUSTER_ADDR: "http://vault:8201"
      VAULT_DEV_ROOT_TOKEN_ID: ${VAULT_ROOT_TOKEN}
      VAULT_DEV_LISTEN_ADDRESS: "0.0.0.0:8200"
      VAULT_LOCAL_CONFIG: |
        {
          "backend": {
            "file": {
              "path": "/vault/data"
            }
          },
          "listener": {
            "tcp": {
              "address": "0.0.0.0:8200",
              "tls_disable": true
            }
          },
          "default_lease_ttl": "168h",
          "max_lease_ttl": "720h",
          "ui": true,
          "log_level": "INFO"
        }
    volumes:
      - vault-data:/vault/data:rw
      - vault-logs:/vault/logs:rw
      - ./vault/config:/vault/config:ro
      - ./vault/policies:/vault/policies:ro
    networks:
      - backend
      - vault-network
    ports:
      - "127.0.0.1:8200:8200"
    healthcheck:
      test: ["CMD", "vault", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=vault"
      - "phase4.vault=enabled"
      - "phase4.model-security=enabled"
    command: >
      vault server
      -config=/vault/config
      -dev-root-token-id=${VAULT_ROOT_TOKEN}
      -dev-listen-address=0.0.0.0:8200

  # ========================================================================
  # Elasticsearch (Hardened)
  # ========================================================================
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.9.2
    container_name: gameforge-elasticsearch-secure
    hostname: elasticsearch
    restart: unless-stopped
    user: "1000:1000"
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - SETUID
      - SETGID
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    environment:
      - node.name=gameforge-elasticsearch
      - cluster.name=gameforge-cluster
      - discovery.type=single-node
      - bootstrap.memory_lock=true
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
      - xpack.security.enabled=true
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - xpack.license.self_generated.type=basic
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data:rw
      - elasticsearch-logs:/usr/share/elasticsearch/logs:rw
      - ./elasticsearch/config/elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml:ro
    networks:
      - backend
      - monitoring
    ports:
      - "127.0.0.1:9200:9200"
    healthcheck:
      test: ["CMD-SHELL", "curl -u elastic:${ELASTIC_PASSWORD} -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=search-engine"
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '0.5'

  # ========================================================================
  # MLflow Model Registry Database (PostgreSQL)
  # ========================================================================
  mlflow-postgres:
    image: postgres:15.4-alpine
    container_name: gameforge-mlflow-postgres-secure
    hostname: mlflow-postgres
    restart: unless-stopped
    <<: [*db-security, *ml-db-resources]
    user: "999:999"
    environment:
      POSTGRES_DB: mlflow
      POSTGRES_USER: mlflow
      POSTGRES_PASSWORD: ${MLFLOW_POSTGRES_PASSWORD}
      PGDATA: /var/lib/postgresql/data/pgdata
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256"
    volumes:
      - mlflow-postgres-data:/var/lib/postgresql/data:rw
      - ./ml-platform/sql/init-mlflow.sql:/docker-entrypoint-initdb.d/01-init-mlflow.sql:ro
      - mlflow-postgres-logs:/var/log/postgresql:rw
    tmpfs:
      - /var/lib/postgresql/data/pg_stat_tmp:noexec,nosuid,size=100m
    networks:
      - ml-network
    ports:
      - "127.0.0.1:5434:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mlflow -d mlflow"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=database"
      - "gameforge.component=mlflow-db"
    command: >
      postgres
      -c ssl=on
      -c ssl_cert_file=/var/lib/postgresql/server.crt
      -c ssl_key_file=/var/lib/postgresql/server.key
      -c shared_preload_libraries=pg_stat_statements
      -c max_connections=100
      -c shared_buffers=512MB
      -c effective_cache_size=1GB
      -c log_statement=mod
      -c log_min_duration_statement=1000

  # ========================================================================
  # MLflow Redis Cache
  # ========================================================================
  mlflow-redis:
    image: redis:7.2.1-alpine
    container_name: gameforge-mlflow-redis-secure
    hostname: mlflow-redis
    restart: unless-stopped
    <<: *db-security
    user: "999:999"
    environment:
      REDIS_PASSWORD: ${MLFLOW_REDIS_PASSWORD}
    volumes:
      - mlflow-redis-data:/data:rw
    networks:
      - ml-network
    ports:
      - "127.0.0.1:6380:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=cache"
      - "gameforge.component=mlflow-cache"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: >
      redis-server
      --requirepass ${MLFLOW_REDIS_PASSWORD}
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru

  # ========================================================================
  # MLflow Tracking Server
  # ========================================================================
  mlflow-server:
    image: gameforge-mlflow:latest
    container_name: gameforge-mlflow-server-secure
    hostname: mlflow-server
    restart: unless-stopped
    <<: [*ml-security, *ml-resources]
    environment:
      <<: *common-env
      # Database Configuration
      MLFLOW_BACKEND_STORE_URI: postgresql://mlflow:${MLFLOW_POSTGRES_PASSWORD}@mlflow-postgres:5432/mlflow
      MLFLOW_DEFAULT_ARTIFACT_ROOT: s3://${MLFLOW_S3_BUCKET}/mlflow-artifacts
      # S3 Configuration for Artifacts
      AWS_ACCESS_KEY_ID: ${MLFLOW_S3_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MLFLOW_S3_SECRET_KEY}
      AWS_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_S3_BUCKET: ${MLFLOW_S3_BUCKET}
      # MLflow Server Configuration
      MLFLOW_HOST: 0.0.0.0
      MLFLOW_PORT: 5000
      MLFLOW_WORKERS: 4
      MLFLOW_GUNICORN_OPTS: "--timeout 300 --keep-alive 2 --max-requests 1000"
      # Security Configuration
      MLFLOW_AUTH_CONFIG_PATH: /app/auth.yaml
      MLFLOW_TRACKING_USERNAME: ${MLFLOW_TRACKING_USERNAME}
      MLFLOW_TRACKING_PASSWORD: ${MLFLOW_TRACKING_PASSWORD}
      # Redis Configuration
      REDIS_URL: redis://:${MLFLOW_REDIS_PASSWORD}@mlflow-redis:6379/0
      # GameForge Integration
      GAMEFORGE_MODEL_REGISTRY_ENABLED: "true"
      GAMEFORGE_SECURITY_ENABLED: "true"
      # OpenTelemetry Configuration for MLflow
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
      OTEL_EXPORTER_OTLP_PROTOCOL: "grpc"
      OTEL_SERVICE_NAME: "gameforge-mlflow-server"
      OTEL_SERVICE_VERSION: ${BUILD_VERSION:-latest}
      OTEL_RESOURCE_ATTRIBUTES: "service.namespace=gameforge,deployment.environment=production,component=mlflow"
      OTEL_TRACES_EXPORTER: "otlp"
      OTEL_METRICS_EXPORTER: "otlp"
      OTEL_PROPAGATORS: "tracecontext,baggage"
    volumes:
      - mlflow-data:/app/data:rw
      - mlflow-logs:/app/logs:rw
      - ./ml-platform/auth.yaml:/app/auth.yaml:ro
      - ./ml-platform/config:/app/config:ro
    networks:
      - ml-network
      - backend
      - monitoring
    ports:
      - "127.0.0.1:5000:5000"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=ml-platform"
      - "gameforge.component=mlflow-server"
      - "traefik.enable=true"
      - "traefik.http.routers.mlflow.rule=Host(`mlflow.gameforge.local`)"
      - "traefik.http.routers.mlflow.tls=true"
      - "traefik.http.services.mlflow.loadbalancer.server.port=5000"
    depends_on:
      mlflow-postgres:
        condition: service_healthy
      mlflow-redis:
        condition: service_healthy

  # ========================================================================
  # MLflow Model Registry (GameForge Custom)
  # ========================================================================
  mlflow-registry:
    image: gameforge-mlflow:latest
    container_name: gameforge-mlflow-registry-secure
    hostname: mlflow-registry
    restart: unless-stopped
    <<: [*ml-security, *ml-resources]
    environment:
      <<: *common-env
      # Database Configuration
      DATABASE_URL: postgresql://mlflow:${MLFLOW_POSTGRES_PASSWORD}@mlflow-postgres:5432/mlflow
      # S3 Configuration
      AWS_ACCESS_KEY_ID: ${MLFLOW_S3_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MLFLOW_S3_SECRET_KEY}
      AWS_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_S3_BUCKET: ${MLFLOW_S3_BUCKET}
      # Registry Configuration
      GAMEFORGE_MODEL_REGISTRY_ENABLED: "true"
      GAMEFORGE_CANARY_DEPLOYMENTS_ENABLED: "true"
      GAMEFORGE_DRIFT_DETECTION_ENABLED: "true"
      # Security Configuration
      GAMEFORGE_SECURITY_ENABLED: "true"
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_ROOT_TOKEN}
      # Redis Configuration
      REDIS_URL: redis://:${MLFLOW_REDIS_PASSWORD}@mlflow-redis:6379/1
      # MLflow Server URL
      MLFLOW_TRACKING_URI: http://mlflow-server:5000
    volumes:
      - mlflow-registry-data:/app/data:rw
      - mlflow-registry-logs:/app/logs:rw
      - ./ml-platform/registry:/app/registry:ro
    networks:
      - ml-network
      - backend
      - vault-network
    ports:
      - "127.0.0.1:5001:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=ml-platform"
      - "gameforge.component=model-registry"
      - "traefik.enable=true"
      - "traefik.http.routers.registry.rule=Host(`registry.gameforge.local`)"
      - "traefik.http.routers.registry.tls=true"
      - "traefik.http.services.registry.loadbalancer.server.port=8080"
    depends_on:
      mlflow-postgres:
        condition: service_healthy
      mlflow-redis:
        condition: service_healthy
      vault:
        condition: service_healthy
    command: ["python", "/app/registry/model_registry.py"]

  # ========================================================================
  # MLflow Canary Deployment Service
  # ========================================================================
  mlflow-canary:
    build:
      context: .
      dockerfile: ml-platform/Dockerfile.canary
    image: gameforge-mlflow-canary:latest
    container_name: gameforge-mlflow-canary-secure
    hostname: mlflow-canary
    restart: unless-stopped
    <<: [*ml-security, *ml-resources]
    environment:
      <<: *common-env
      # Database Configuration
      DATABASE_URL: postgresql://mlflow:${MLFLOW_POSTGRES_PASSWORD}@mlflow-postgres:5432/mlflow
      # S3 Configuration
      AWS_ACCESS_KEY_ID: ${MLFLOW_S3_ACCESS_KEY}
      AWS_SECRET_ACCESS_KEY: ${MLFLOW_S3_SECRET_KEY}
      AWS_ENDPOINT_URL: ${S3_ENDPOINT_URL}
      AWS_REGION: ${AWS_REGION:-us-east-1}
      AWS_S3_BUCKET: ${MLFLOW_S3_BUCKET}
      # Canary Configuration
      GAMEFORGE_CANARY_ENABLED: "true"
      GAMEFORGE_AUTO_PROMOTE: "false"
      GAMEFORGE_AUTO_ROLLBACK: "true"
      GAMEFORGE_TRAFFIC_INCREMENT: "5"
      GAMEFORGE_MAX_TRAFFIC: "50"
      GAMEFORGE_MONITORING_DURATION: "30"
      # Security Configuration
      GAMEFORGE_SECURITY_ENABLED: "true"
      VAULT_ADDR: http://vault:8200
      VAULT_TOKEN: ${VAULT_ROOT_TOKEN}
      # Redis Configuration
      REDIS_URL: redis://:${MLFLOW_REDIS_PASSWORD}@mlflow-redis:6379/2
      # MLflow Server URL
      MLFLOW_TRACKING_URI: http://mlflow-server:5000
      # Prometheus Configuration
      PROMETHEUS_METRICS_PORT: "8080"
    volumes:
      - mlflow-canary-data:/app/data:rw
      - mlflow-canary-logs:/app/logs:rw
      - ./ml-platform/config:/app/config:ro
    networks:
      - ml-network
      - backend
      - monitoring
      - vault-network
    ports:
      - "127.0.0.1:5002:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 120s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=ml-platform"
      - "gameforge.component=canary-deployment"
      - "traefik.enable=true"
      - "traefik.http.routers.canary.rule=Host(`canary.gameforge.local`)"
      - "traefik.http.routers.canary.tls=true"
      - "traefik.http.services.canary.loadbalancer.server.port=8080"
    depends_on:
      mlflow-postgres:
        condition: service_healthy
      mlflow-redis:
        condition: service_healthy
      mlflow-server:
        condition: service_healthy
      vault:
        condition: service_healthy

  # ========================================================================
  # Logstash Log Processing (Hardened)
  # ========================================================================
  logstash:
    image: docker.elastic.co/logstash/logstash:8.9.2
    container_name: gameforge-logstash-secure
    hostname: logstash
    restart: unless-stopped
    user: "1000:1000"
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    environment:
      - "LS_JAVA_OPTS=-Xmx1g -Xms1g"
      - LOGSTASH_SYSTEM_PASSWORD=${LOGSTASH_SYSTEM_PASSWORD}
      - xpack.monitoring.enabled=true
      - xpack.monitoring.elasticsearch.hosts=["elasticsearch:9200"]
      - xpack.monitoring.elasticsearch.username=logstash_system
      - xpack.monitoring.elasticsearch.password=${LOGSTASH_SYSTEM_PASSWORD}
    volumes:
      - ./monitoring/logging/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf:ro
      - logstash-data:/usr/share/logstash/data:rw
    networks:
      - backend
      - monitoring
    ports:
      - "127.0.0.1:5044:5044"  # Beats input
      - "127.0.0.1:8080:8080"  # HTTP input
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9600/_node/stats"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=log-processor"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.2'
    depends_on:
      elasticsearch:
        condition: service_healthy

  # ========================================================================
  # Filebeat Log Collection (Hardened)
  # ========================================================================
  filebeat:
    image: docker.elastic.co/beats/filebeat:8.9.2
    container_name: gameforge-filebeat-secure
    hostname: filebeat
    restart: unless-stopped
    user: "root"  # Required for log file access
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /usr/share/filebeat/data:noexec,nosuid,size=500m
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
    command: [
      "--strict.perms=false",
      "-e",
      "-E", "output.logstash.hosts=[\"logstash:5044\"]",
      "-E", "setup.kibana.host=grafana:3000"
    ]
    environment:
      - FILEBEAT_SYSTEM_PASSWORD=${FILEBEAT_SYSTEM_PASSWORD}
      - NODE_NAME=gameforge-filebeat
    volumes:
      - ./monitoring/logging/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - gameforge-logs:/var/log/gameforge:ro
      - nginx-logs:/var/log/nginx:ro
      - postgres-logs:/var/log/postgresql:ro
      - elasticsearch-logs:/var/log/elasticsearch:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - backend
      - monitoring
    healthcheck:
      test: ["CMD", "filebeat", "test", "config"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=log-collector"
      - "co.elastic.logs/enabled=false"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'
    depends_on:
      logstash:
        condition: service_healthy

  # ========================================================================
  # Background Workers (Hardened)
  # ========================================================================
  gameforge-worker:
    build:
      context: .
      dockerfile: Dockerfile.production
      target: production
    image: gameforge:production-secure
    container_name: gameforge-worker-secure
    hostname: gameforge-worker
    restart: unless-stopped
    <<: *app-security
    # GPU optimization for AI worker tasks
    runtime: nvidia
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
          pids: 500
        reservations:
          memory: 1G
          cpus: '0.5'
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 3
    environment:
      <<: *common-env
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/0
      ELASTICSEARCH_URL: http://elasticsearch:9200
      WORKER_TYPE: background
      CELERY_BROKER_URL: redis://redis:6379/1
      CELERY_RESULT_BACKEND: redis://redis:6379/2
      # GPU optimization environment for workers
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility
      PYTORCH_CUDA_ALLOC_CONF: max_split_size_mb:256,garbage_collection_threshold:0.6,expandable_segments:True
      CUDA_LAUNCH_BLOCKING: 0
      CUDA_CACHE_DISABLE: 0
      PYTORCH_JIT: 1
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      - gameforge-models:/app/models_cache:rw
    networks:
      - backend
      - monitoring
    healthcheck:
      test: ["CMD", "python", "-c", "import celery; print('Worker healthy')"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=worker"
      - "ai.gpu.enabled=true"
      - "ai.gpu.driver=nvidia"
    command: ["celery", "worker", "-A", "gameforge.celery", "--loglevel=info", "--concurrency=4"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  # ========================================================================
  # GPU Inference Service - High-Performance AI Workload Processing
  # ========================================================================
  gameforge-gpu-inference:
    build:
      context: .
      dockerfile: Dockerfile.gpu-workload
      target: gpu-production
      args:
        BUILD_DATE: ${BUILD_DATE:-}
        VCS_REF: ${VCS_REF:-}
        BUILD_VERSION: ${BUILD_VERSION:-latest}
        CUDA_VERSION: ${CUDA_VERSION:-12.1}
        CUDNN_VERSION: ${CUDNN_VERSION:-8}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.10}
    image: gameforge:gpu-inference-production
    container_name: gameforge-gpu-inference
    hostname: gameforge-gpu-inference
    restart: unless-stopped
    runtime: nvidia
    <<: [*app-security, *gpu-inference-resources]
    environment:
      <<: *common-env
      SERVICE_TYPE: gpu-inference
      WORKER_TYPE: gpu-inference
      GPU_MEMORY_FRACTION: "0.8"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:1024,garbage_collection_threshold:0.8,expandable_segments:True"
      CUDA_LAUNCH_BLOCKING: "0"
      CUDA_CACHE_DISABLE: "0"
      PYTORCH_JIT: "1"
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-all}
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      NVIDIA_REQUIRE_CUDA: "cuda>=12.0"
      MODEL_CACHE_SIZE: "8G"
      BATCH_SIZE: ${INFERENCE_BATCH_SIZE:-4}
      MAX_SEQUENCE_LENGTH: ${MAX_SEQUENCE_LENGTH:-2048}
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/3
      CELERY_BROKER_URL: redis://redis:6379/4
      CELERY_RESULT_BACKEND: redis://redis:6379/5
      # OpenTelemetry Configuration for GPU Inference
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
      OTEL_EXPORTER_OTLP_PROTOCOL: "grpc"
      OTEL_SERVICE_NAME: "gameforge-gpu-inference"
      OTEL_SERVICE_VERSION: ${BUILD_VERSION:-latest}
      OTEL_RESOURCE_ATTRIBUTES: "service.namespace=gameforge,deployment.environment=production,gpu.workload=inference"
      OTEL_TRACES_EXPORTER: "otlp"
      OTEL_METRICS_EXPORTER: "otlp"
      OTEL_PROPAGATORS: "tracecontext,baggage"
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      - model-cache:/app/models_cache:rw
      - gpu-shared-memory:/dev/shm:rw
    networks:
      - backend
      - monitoring
      - gpu-network
    ports:
      - "127.0.0.1:8081:8080"
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; print(f'GPU available: {torch.cuda.is_available()}'); exit(0 if torch.cuda.is_available() else 1)"]
      interval: 60s
      timeout: 30s
      retries: 3
      start_period: 180s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=gpu-worker"
      - "ai.gpu.enabled=true"
      - "ai.gpu.driver=nvidia"
      - "ai.gpu.workload=inference"
      - "ai.gpu.priority=high"
    command: ["python", "-m", "gameforge.services.gpu_inference_service"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      gameforge-app:
        condition: service_healthy
      otel-collector:
        condition: service_healthy

  # ========================================================================
  # GPU Training Service - Model Training and Fine-tuning
  # ========================================================================
  gameforge-gpu-training:
    build:
      context: .
      dockerfile: Dockerfile.gpu-workload
      target: gpu-production
      args:
        BUILD_DATE: ${BUILD_DATE:-}
        VCS_REF: ${VCS_REF:-}
        BUILD_VERSION: ${BUILD_VERSION:-latest}
        CUDA_VERSION: ${CUDA_VERSION:-12.1}
        CUDNN_VERSION: ${CUDNN_VERSION:-8}
        PYTHON_VERSION: ${PYTHON_VERSION:-3.10}
    image: gameforge:gpu-training-production
    container_name: gameforge-gpu-training
    hostname: gameforge-gpu-training
    restart: unless-stopped
    runtime: nvidia
    <<: [*app-security, *gpu-training-resources]
    environment:
      <<: *common-env
      SERVICE_TYPE: gpu-training
      WORKER_TYPE: gpu-training
      GPU_MEMORY_FRACTION: "0.95"
      PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:2048,garbage_collection_threshold:0.9,expandable_segments:True"
      CUDA_LAUNCH_BLOCKING: "0"
      CUDA_CACHE_DISABLE: "0"
      PYTORCH_JIT: "1"
      NVIDIA_VISIBLE_DEVICES: ${TRAINING_NVIDIA_VISIBLE_DEVICES:-all}
      NVIDIA_DRIVER_CAPABILITIES: "compute,utility"
      NVIDIA_REQUIRE_CUDA: "cuda>=12.0"
      MODEL_CACHE_SIZE: "16G"
      TRAINING_BATCH_SIZE: ${TRAINING_BATCH_SIZE:-8}
      GRADIENT_ACCUMULATION_STEPS: ${GRADIENT_ACCUMULATION_STEPS:-4}
      MIXED_PRECISION: ${MIXED_PRECISION:-fp16}
      DATABASE_URL: postgresql://gameforge:${POSTGRES_PASSWORD}@postgres:5432/gameforge_prod
      REDIS_URL: redis://redis:6379/6
      CELERY_BROKER_URL: redis://redis:6379/7
      CELERY_RESULT_BACKEND: redis://redis:6379/8
      # OpenTelemetry Configuration for GPU Training
      OTEL_EXPORTER_OTLP_ENDPOINT: "http://otel-collector:4317"
      OTEL_EXPORTER_OTLP_PROTOCOL: "grpc"
      OTEL_SERVICE_NAME: "gameforge-gpu-training"
      OTEL_SERVICE_VERSION: ${BUILD_VERSION:-latest}
      OTEL_RESOURCE_ATTRIBUTES: "service.namespace=gameforge,deployment.environment=production,gpu.workload=training"
      OTEL_TRACES_EXPORTER: "otlp"
      OTEL_METRICS_EXPORTER: "otlp"
      OTEL_PROPAGATORS: "tracecontext,baggage"
    volumes:
      - gameforge-logs:/app/logs:rw
      - gameforge-cache:/app/cache:rw
      - gameforge-assets:/app/generated_assets:rw
      - model-cache:/app/models_cache:rw
      - training-data:/app/training_data:rw
      - model-checkpoints:/app/checkpoints:rw
      - gpu-shared-memory:/dev/shm:rw
    networks:
      - backend
      - monitoring
      - gpu-network
    ports:
      - "127.0.0.1:8082:8080"
    healthcheck:
      test: ["CMD", "python", "-c", "import torch; print(f'GPUs available: {torch.cuda.device_count()}'); exit(0 if torch.cuda.device_count() >= 2 else 1)"]
      interval: 120s
      timeout: 60s
      retries: 2
      start_period: 300s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=gpu-worker"
      - "ai.gpu.enabled=true"
      - "ai.gpu.driver=nvidia"
      - "ai.gpu.workload=training"
      - "ai.gpu.priority=highest"
    command: ["python", "-m", "gameforge.services.gpu_training_service"]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      gameforge-gpu-inference:
        condition: service_healthy
      otel-collector:
        condition: service_healthy

  # ========================================================================
  # OpenTelemetry Collector (Distributed Tracing)
  # ========================================================================
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.88.0
    container_name: gameforge-otel-collector-secure
    hostname: otel-collector
    restart: unless-stopped
    user: "10001:10001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    environment:
      - OTEL_LOG_LEVEL=info
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:14250
      - OTEL_RESOURCE_ATTRIBUTES=service.name=gameforge-otel-collector,service.version=1.0.0
    volumes:
      - ../../monitoring/otel/otel-collector-config.yaml:/etc/otelcol-contrib/otel-collector-config.yaml:ro
      - otel-collector-data:/data:rw
    networks:
      - monitoring
      - backend
    ports:
      - "127.0.0.1:4317:4317"   # OTLP gRPC receiver
      - "127.0.0.1:4318:4318"   # OTLP HTTP receiver
      - "127.0.0.1:8888:8888"   # Prometheus metrics
      - "127.0.0.1:8889:8889"   # Prometheus exporter metrics
      - "127.0.0.1:13133:13133" # Health check
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:13133/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=observability"
      - "observability.component=tracing-collector"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: ["--config=/etc/otelcol-contrib/otel-collector-config.yaml"]

  # ========================================================================
  # Jaeger All-in-One (Distributed Tracing Backend)
  # ========================================================================
  jaeger:
    image: jaegertracing/all-in-one:1.50.0
    container_name: gameforge-jaeger-secure
    hostname: jaeger
    restart: unless-stopped
    user: "10001:10001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=200m
      - /badger-tmp:noexec,nosuid,size=1G  # In-memory storage for traces
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    environment:
      - SPAN_STORAGE_TYPE=badger
      - BADGER_EPHEMERAL=true
      - BADGER_DIRECTORY_VALUE=/badger-tmp/data
      - BADGER_DIRECTORY_KEY=/badger-tmp/key
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
      - METRICS_STORAGE_TYPE=prometheus
      - PROMETHEUS_SERVER_URL=http://prometheus:9090
      - JAEGER_DISABLED=false
      - LOG_LEVEL=info
      # GameForge specific configuration
      - QUERY_MAX_CLOCK_SKEW_ADJUSTMENT=0s
      - QUERY_TIMEOUT=30s
      - QUERY_MAX_TRACES=500
    volumes:
      - jaeger-data:/badger-persistent:rw
    networks:
      - monitoring
      - backend
    ports:
      - "127.0.0.1:16686:16686" # Jaeger UI
      - "127.0.0.1:14250:14250" # gRPC
      - "127.0.0.1:14268:14268" # HTTP
      - "127.0.0.1:9411:9411"   # Zipkin compatible endpoint
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:14269/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=observability"
      - "observability.component=tracing-backend"
      - "traefik.enable=true"
      - "traefik.http.routers.jaeger.rule=Host(`jaeger.gameforge.local`)"
      - "traefik.http.routers.jaeger.tls=true"
      - "traefik.http.services.jaeger.loadbalancer.server.port=16686"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.2'

  # ========================================================================
  # Backup Service (Hardened)
  # ========================================================================
  backup-service:
    image: postgres:15.4-alpine
    container_name: gameforge-backup-secure
    hostname: backup-service
    restart: unless-stopped
    user: "1001:1001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
      - /var/run:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/database.json
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETUID
      - SETGID
    environment:
      POSTGRES_DB: gameforge_prod
      POSTGRES_USER: gameforge
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      PGPASSWORD: ${POSTGRES_PASSWORD}
      BACKUP_SCHEDULE: "0 2 * * *"  # Daily at 2 AM
      BACKUP_RETENTION_DAYS: 30
      S3_BUCKET: ${BACKUP_S3_BUCKET}
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY}
    volumes:
      - backup-data:/backups:rw
      - ./scripts/backup.sh:/usr/local/bin/backup.sh:ro
      - ./scripts/restore.sh:/usr/local/bin/restore.sh:ro
    networks:
      - backend
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=backup"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    command: ["sh", "-c", "crond -f -d 8"]
    depends_on:
      postgres:
        condition: service_healthy

  # ========================================================================
  # AlertManager (Prometheus Alerting)
  # ========================================================================
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: gameforge-alertmanager-secure
    hostname: alertmanager
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /alertmanager-tmp:noexec,nosuid,size=200m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    environment:
      - ALERTMANAGER_WEB_EXTERNAL_URL=http://alertmanager.gameforge.local:9093
      - ALERTMANAGER_CLUSTER_LISTEN_ADDRESS=0.0.0.0:9094
      - ALERTMANAGER_LOG_LEVEL=info
      # Environment variables for AlertManager configuration 
      # Security fix: Remove test credential defaults
      - SMTP_HOST=${SMTP_HOST}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - EMAIL_FROM=${EMAIL_FROM}
      - ALERT_EMAIL_DEFAULT=${ALERT_EMAIL_DEFAULT}
      - ALERT_EMAIL_CRITICAL=${ALERT_EMAIL_CRITICAL}
      - ALERT_EMAIL_SECURITY=${ALERT_EMAIL_SECURITY}
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
      - PAGERDUTY_ROUTING_KEY=${PAGERDUTY_ROUTING_KEY}
      - PAGERDUTY_API_KEY=${PAGERDUTY_API_KEY}
    volumes:
      - ../../monitoring/alertmanager/alertmanager.production.fixed.yaml:/etc/alertmanager/alertmanager.yaml:ro
      - ../../monitoring/alertmanager/templates:/etc/alertmanager/templates:ro
      - alertmanager-data:/alertmanager:rw
    networks:
      - monitoring
      - backend
    ports:
      - "127.0.0.1:9093:9093"   # AlertManager UI
      - "127.0.0.1:9094:9094"   # Cluster communication
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yaml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://alertmanager.gameforge.local:9093'
      - '--web.route-prefix=/'
      - '--cluster.listen-address=0.0.0.0:9094'
      - '--log.level=info'
      - '--data.retention=120h'
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=monitoring"
      - "monitoring.component=alerting"
      - "traefik.enable=true"
      - "traefik.http.routers.alertmanager.rule=Host(`alertmanager.gameforge.local`)"
      - "traefik.http.routers.alertmanager.tls=true"
      - "traefik.http.services.alertmanager.loadbalancer.server.port=9093"
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.5'
        reservations:
          memory: 128M
          cpus: '0.1'

  # ========================================================================
  # Prometheus Monitoring (Hardened) - Updated with AlertManager integration
  # ========================================================================
  prometheus:
    image: prom/prometheus:v2.47.0
    container_name: gameforge-prometheus-secure
    hostname: prometheus
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    volumes:
      - ../../monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus:rw
    networks:
      - monitoring
    ports:
      - "127.0.0.1:9090:9090"
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--alertmanager.timeout=10s'
      - '--query.timeout=2m'
      - '--web.external-url=http://prometheus.gameforge.local:9090'
    depends_on:
      alertmanager:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=monitoring"
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.2'

  # ========================================================================
  # Notification Service (PagerDuty, Slack, Email Integration)
  # ========================================================================
  notification-service:
    build:
      context: ./monitoring/notifications
      dockerfile: Dockerfile
    image: gameforge-notifications:latest
    container_name: gameforge-notification-service
    hostname: notification-service
    restart: unless-stopped
    user: "1001:1001"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    environment:
      # Webhook endpoints
      - SLACK_WEBHOOK_URL=${SLACK_WEBHOOK_URL}
      - DISCORD_WEBHOOK_URL=${DISCORD_WEBHOOK_URL}
      - TEAMS_WEBHOOK_URL=${TEAMS_WEBHOOK_URL}
      
      # Email configuration
      - SMTP_HOST=${SMTP_HOST}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_USERNAME=${SMTP_USERNAME}
      - SMTP_PASSWORD=${SMTP_PASSWORD}
      - EMAIL_FROM=${EMAIL_FROM}
      
      # PagerDuty integration
      - PAGERDUTY_ROUTING_KEY=${PAGERDUTY_ROUTING_KEY}
      - PAGERDUTY_API_KEY=${PAGERDUTY_API_KEY}
      
      # Service configuration
      - LOG_LEVEL=info
      - NOTIFICATION_TIMEOUT=30s
      - RETRY_ATTEMPTS=3
      - GAMEFORGE_ENV=production
    volumes:
      - notification-templates:/app/templates:ro
      - notification-logs:/app/logs:rw
    networks:
      - monitoring
      - backend
    ports:
      - "127.0.0.1:8085:8080"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=notification"
      - "monitoring.component=alerting"
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: '0.25'
        reservations:
          memory: 64M
          cpus: '0.05'

  # ========================================================================
  # Grafana Dashboard (Hardened) - Updated with AlertManager integration
  # ========================================================================
  grafana:
    image: grafana/grafana:10.1.2
    container_name: gameforge-grafana-secure
    hostname: grafana
    restart: unless-stopped
    user: "472:472"
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/lib/grafana/plugins:noexec,nosuid,size=100m
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    cap_drop:
      - ALL
    environment:
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY}
      GF_USERS_ALLOW_SIGN_UP: false
      GF_SECURITY_DISABLE_GRAVATAR: true
      GF_ANALYTICS_REPORTING_ENABLED: false
      GF_ANALYTICS_CHECK_FOR_UPDATES: false
      GF_SECURITY_COOKIE_SECURE: true
      GF_SECURITY_COOKIE_SAMESITE: strict
      GF_SERVER_ROOT_URL: https://${DOMAIN}/grafana/
    volumes:
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards:ro
      - grafana-data:/var/lib/grafana:rw
    networks:
      - monitoring
      - frontend
    ports:
      - "127.0.0.1:3000:3000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=dashboard"
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    depends_on:
      prometheus:
        condition: service_healthy
      jaeger:
        condition: service_healthy
      alertmanager:
        condition: service_healthy

  # ========================================================================
  # Phase 3: Image Security Pipeline Services
  # ========================================================================

  # Security Scanner Service (Trivy + Grype)
  security-scanner:
    image: aquasec/trivy:latest
    container_name: gameforge-security-scanner
    command: ["server", "--listen", "0.0.0.0:8080"]
    environment:
      TRIVY_LISTEN: "0.0.0.0:8080"
      TRIVY_CACHE_DIR: "/cache"
      TRIVY_DB_REPOSITORY: "ghcr.io/aquasecurity/trivy-db"
      TRIVY_JAVA_DB_REPOSITORY: "ghcr.io/aquasecurity/trivy-java-db"
      TRIVY_LOG_LEVEL: "INFO"
      # Phase 3: Security scanning configuration
      TRIVY_SEVERITY: "UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL"
      TRIVY_SECURITY_CHECKS: "vuln,secret,config"
      TRIVY_TIMEOUT: "10m"
    volumes:
      - security-scanner-cache:/cache:rw
      - security-scanner-reports:/reports:rw
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./security/scripts:/scripts:ro
    networks:
      - security-pipeline
      - monitoring
    ports:
      - "127.0.0.1:8082:8080"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/healthz"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=scanner"
      - "security.component=phase3-pipeline"

  # SBOM Generator Service
  sbom-generator:
    image: anchore/syft:latest
    container_name: gameforge-sbom-generator
    command: ["serve", "--host", "0.0.0.0", "--port", "8080"]
    environment:
      SYFT_LOG_LEVEL: "info"
      SYFT_QUIET: "false"
      # Phase 3: SBOM generation configuration
      SYFT_OUTPUT_FORMAT: "spdx-json"
      SYFT_CATALOGERS: "all"
      SYFT_SCOPE: "all-layers"
    volumes:
      - sbom-reports:/reports:rw
      - sbom-cache:/cache:rw
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./security/scripts:/scripts:ro
    networks:
      - security-pipeline
      - monitoring
    ports:
      - "127.0.0.1:8083:8080"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=sbom"
      - "security.component=phase3-pipeline"

  # Image Signing Service (Cosign)
  image-signer:
    image: gcr.io/projectsigstore/cosign:latest
    container_name: gameforge-image-signer
    command: ["sleep", "infinity"]
    environment:
      COSIGN_EXPERIMENTAL: "1"
      COSIGN_PRIVATE_KEY: "/keys/cosign.key"
      COSIGN_PUBLIC_KEY: "/keys/cosign.pub"
      # Phase 3: Image signing configuration
      SIGSTORE_ROOT_FILE: "/etc/ssl/certs/fulcio-root.crt"
      REKOR_URL: "https://rekor.sigstore.dev"
    volumes:
      - cosign-keys:/keys:ro
      - signing-reports:/reports:rw
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./security/scripts:/scripts:ro
    networks:
      - security-pipeline
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=200m
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: '0.25'
        reservations:
          memory: 128M
          cpus: '0.05'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=signer"
      - "security.component=phase3-pipeline"

  # Harbor Registry (Enterprise Container Registry)
  harbor-registry:
    image: goharbor/harbor-core:latest
    container_name: gameforge-harbor-registry
    environment:
      CORE_SECRET: ${HARBOR_CORE_SECRET}
      JOBSERVICE_SECRET: ${HARBOR_JOBSERVICE_SECRET}
      # Phase 3: Harbor configuration
      HARBOR_ADMIN_PASSWORD: ${HARBOR_ADMIN_PASSWORD}
      DATABASE_TYPE: "postgresql"
      DATABASE_HOSTNAME: "postgres"
      DATABASE_USERNAME: ${POSTGRES_USER}
      DATABASE_PASSWORD: ${POSTGRES_PASSWORD}
      DATABASE_DATABASE: "harbor_registry"
      DATABASE_SSL_MODE: "require"
      REDIS_URL: "redis://redis:6379/2"
      CLAIR_DB_HOST: "postgres"
      CLAIR_DB_PASSWORD: ${POSTGRES_PASSWORD}
      CLAIR_DB_USERNAME: ${POSTGRES_USER}
      CLAIR_DB: "harbor_clair"
      CLAIR_URL: "http://clair-scanner:6060"
      NOTARY_URL: "http://notary-server:4443"
      REGISTRY_URL: "http://registry:5000"
      TOKEN_SERVICE_URL: "http://harbor-registry:8080/service/token"
      WITH_NOTARY: "true"
      WITH_CLAIR: "true"
      WITH_CHARTMUSEUM: "false"
      LOG_LEVEL: "info"
      CONFIG_PATH: "/etc/harbor"
      SHARED_PATH: "/shared"
      REGISTRY_CONTROLLER_URL: "http://registryctl:8080"
      PORTAL_URL: "http://harbor-portal:8080"
      REGISTRYCTL_URL: "http://registryctl:8080"
    volumes:
      - harbor-registry-data:/data:rw
      - harbor-registry-logs:/var/log/harbor:rw
      - ./security/configs/harbor.yml:/etc/harbor/harbor.yml:ro
      - harbor-registry-secret:/etc/harbor/secretkey:rw
      - harbor-registry-ca:/etc/harbor/ca:ro
    networks:
      - security-pipeline
      - backend
      - monitoring
    ports:
      - "127.0.0.1:8084:8080"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/v2.0/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
      - seccomp=../../security/seccomp/gameforge-app.json
    read_only: true
    tmpfs:
      - /tmp:noexec,nosuid,size=500m
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.25'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=registry"
      - "security.component=phase3-pipeline"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  # Security Dashboard (Grafana with Security Templates)
  security-dashboard:
    image: grafana/grafana:latest
    container_name: gameforge-security-dashboard
    hostname: security-dashboard
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: admin
      # Security fix: Remove default password
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_ADMIN_PASSWORD}
      GF_SECURITY_SECRET_KEY: ${GRAFANA_SECRET_KEY:-}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_USERS_ALLOW_ORG_CREATE: "false"
      GF_AUTH_ANONYMOUS_ENABLED: "false"
      # Phase 3: Security dashboard configuration
      GF_INSTALL_PLUGINS: "grafana-clock-panel,grafana-simple-json-datasource"
      GF_RENDERING_SERVER_URL: "http://localhost:8081/render"
      GF_LOG_LEVEL: "info"
      GF_METRICS_ENABLED: "true"
      GF_SECURITY_DISABLE_GRAVATAR: "true"
      GF_SECURITY_COOKIE_SECURE: "true"
      GF_SECURITY_STRICT_TRANSPORT_SECURITY: "true"
      GF_SECURITY_CONTENT_TYPE_PROTECTION: "true"
      GF_ANALYTICS_REPORTING_ENABLED: "false"
      GF_ANALYTICS_CHECK_FOR_UPDATES: "false"
      GF_SNAPSHOTS_EXTERNAL_ENABLED: "false"
    volumes:
      - security-dashboard-data:/var/lib/grafana:rw
      - ./security/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./security/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - security-pipeline
      - monitoring
    ports:
      - "127.0.0.1:3001:3000"
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    security_opt:
      - no-new-privileges:true
    read_only: false  # Grafana needs write access for dashboard storage
    tmpfs:
      - /tmp:noexec,nosuid,size=200m
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.1'
    logging: *secure-logging
    labels:
      - "security.scan=enabled"
      - "security.profile=dashboard"
      - "security.component=phase3-pipeline"

  # Dataset Versioning Service with DVC
  dataset-api:
    build:
      context: ../../ml-platform/data
      dockerfile: Dockerfile
    container_name: gameforge-dataset-api
    hostname: dataset-api
    restart: unless-stopped
    env_file:
      - .env
    environment:
      # Database configuration
      - POSTGRES_HOST=mlflow-postgres
      - POSTGRES_PORT=5432
      - POSTGRES_DB=mlflow
      - POSTGRES_USER=mlflow
      - POSTGRES_PASSWORD=mlflow_password
      
      # Redis configuration
      - REDIS_URL=redis://mlflow-redis:6379/3
      
      # DVC configuration
      - DVC_REMOTE_URL=s3://gameforge-datasets
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION:-us-west-2}
      
      # API configuration
      - API_HOST=0.0.0.0
      - API_PORT=8080
      - LOG_LEVEL=INFO
      
      # Security
      - API_CORS_ORIGINS=["http://localhost:3000","https://gameforge.local"]
    ports:
      - "8080:8080"
    volumes:
      - dataset_cache:/app/data
    tmpfs:
      - /tmp/dataset:size=100M
    networks:
      - ml-network
    depends_on:
      - mlflow-postgres
      - mlflow-redis
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.dataset-api.rule=Host(`dataset-api.gameforge.local`)"
      - "traefik.http.routers.dataset-api.tls=true"
      - "traefik.http.services.dataset-api.loadbalancer.server.port=8080"
      - "security.scan=enabled"
      - "security.profile=api"
      - "security.component=dataset-versioning"
    <<: *ml-security
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # ========================================================================
  # AI PLATFORM SERVICES - Ray, KubeFlow, TorchServe for vast.ai RTX 4090
  # ========================================================================
  
  # TorchServe - Model serving optimized for RTX 4090 (24GB VRAM)
  torchserve-rtx4090:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.torchserve
      args:
        - CUDA_VERSION=12.1
        - TORCH_VERSION=2.1.0
    environment:
      # TorchServe configuration
      - TS_CONFIG_FILE=/opt/torchserve/ts.config
      - TS_MODEL_STORE=/opt/torchserve/model-store
      - TS_LOG_LOCATION=/opt/torchserve/logs
      - JAVA_OPTS=-Xms2g -Xmx8g
      
      # GPU optimization for RTX 4090
      - CUDA_VISIBLE_DEVICES=0
      - TORCH_CUDA_ARCH_LIST=8.9
      - PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:2048,expandable_segments:True
      
      # Model configuration
      - MODEL_STORE_PATH=/opt/torchserve/model-store
      - METRICS_ENABLED=true
      - METRICS_MODE=prometheus
    ports:
      - "8080:8080"  # Inference API
      - "8081:8081"  # Management API
      - "8082:8082"  # Metrics API
    volumes:
      - torchserve_models:/opt/torchserve/model-store
      - torchserve_logs:/opt/torchserve/logs
      - ./configs/torchserve:/opt/torchserve/config:ro
    networks:
      - ai-network
      - monitoring-network
    devices:
      - "/dev/nvidia0:/dev/nvidia0"
      - "/dev/nvidiactl:/dev/nvidiactl"
      - "/dev/nvidia-uvm:/dev/nvidia-uvm"
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.torchserve.rule=Host(`torchserve.gameforge.local`)"
      - "traefik.http.routers.torchserve.tls=true"
      - "traefik.http.services.torchserve.loadbalancer.server.port=8080"
      - "security.scan=enabled"
      - "security.profile=gpu-inference"
      - "security.component=torchserve"
    <<: [*app-security, *gpu-inference-resources]

  # Ray Head Node - Distributed computing cluster head
  ray-head-rtx4090:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.ray
      args:
        - PYTHON_VERSION=3.11
        - RAY_VERSION=2.8.1
    environment:
      # Ray cluster configuration
      - RAY_HEAD_NODE=true
      - RAY_REDIS_ADDRESS=ray-head-rtx4090:6379
      - RAY_OBJECT_STORE_MEMORY=4000000000
      - RAY_PLASMA_DIRECTORY=/tmp/ray
      
      # GPU configuration
      - CUDA_VISIBLE_DEVICES=0
      - RAY_NUM_GPUS=1
      - RAY_GPU_MEMORY_FRACTION=0.8
      
      # Dashboard and metrics
      - RAY_DASHBOARD_HOST=0.0.0.0
      - RAY_METRICS_EXPORT_PORT=8080
    ports:
      - "8265:8265"  # Ray Dashboard
      - "10001:10001"  # Ray Client
      - "6379:6379"   # Redis
    volumes:
      - ray_logs:/tmp/ray
      - ray_data:/data
      - ./src/ai:/workspace/ai:ro
    networks:
      - ai-network
      - monitoring-network
    devices:
      - "/dev/nvidia0:/dev/nvidia0"
      - "/dev/nvidiactl:/dev/nvidiactl"
      - "/dev/nvidia-uvm:/dev/nvidia-uvm"
    runtime: nvidia
    command: >
      ray start --head 
      --dashboard-host=0.0.0.0 
      --dashboard-port=8265
      --redis-port=6379
      --object-store-memory=4000000000
      --num-cpus=16
      --num-gpus=1
      --block
    healthcheck:
      test: ["CMD", "ray", "status"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.ray-dashboard.rule=Host(`ray.gameforge.local`)"
      - "traefik.http.routers.ray-dashboard.tls=true"
      - "traefik.http.services.ray-dashboard.loadbalancer.server.port=8265"
      - "security.scan=enabled"
      - "security.profile=gpu-training"
      - "security.component=ray-cluster"
    <<: [*app-security, *gpu-training-resources]

  # KubeFlow Pipelines - ML workflow orchestration
  kubeflow-pipelines-rtx4090:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.kubeflow
      args:
        - KUBEFLOW_VERSION=1.8.5
    environment:
      # KubeFlow configuration
      - KUBEFLOW_USERID=user@gameforge.local
      - MINIO_ENDPOINT_URL=http://minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      
      # MySQL connection
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_DATABASE=kubeflow
      - MYSQL_USER=${MYSQL_USER}
      - MYSQL_PASSWORD=${MYSQL_PASSWORD}
      
      # GPU settings
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8888:8888"  # KubeFlow UI
      - "3000:3000"  # API Server
    volumes:
      - kubeflow_pipelines:/opt/kubeflow/pipelines
      - kubeflow_metadata:/opt/kubeflow/metadata
      - ./src/ml/pipelines:/workspace/pipelines:ro
    networks:
      - ai-network
      - backend-network
      - monitoring-network
    depends_on:
      - mysql
      - minio
      - ray-head-rtx4090
    devices:
      - "/dev/nvidia0:/dev/nvidia0"
      - "/dev/nvidiactl:/dev/nvidiactl"
      - "/dev/nvidia-uvm:/dev/nvidia-uvm"
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/pipeline"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 180s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kubeflow.rule=Host(`kubeflow.gameforge.local`)"
      - "traefik.http.routers.kubeflow.tls=true"
      - "traefik.http.services.kubeflow.loadbalancer.server.port=8888"
      - "security.scan=enabled"
      - "security.profile=ml-platform"
      - "security.component=kubeflow"
    <<: [*app-security, *gpu-training-resources]

  # DCGM Exporter - GPU monitoring for RTX 4090
  dcgm-exporter-rtx4090:
    image: nvcr.io/nvidia/k8s/dcgm-exporter:3.3.0-3.2.0-ubuntu22.04
    environment:
      - DCGM_EXPORTER_LISTEN=:9400
      - DCGM_EXPORTER_KUBERNETES=false
      - DCGM_EXPORTER_COLLECTORS=/etc/dcgm-exporter/dcp-metrics-included.csv
    ports:
      - "9400:9400"
    volumes:
      - /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:/usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1:ro
      - ./configs/dcgm/dcp-metrics-included.csv:/etc/dcgm-exporter/dcp-metrics-included.csv:ro
    networks:
      - monitoring-network
    devices:
      - "/dev/nvidia0:/dev/nvidia0"
      - "/dev/nvidiactl:/dev/nvidiactl"
      - "/dev/nvidia-uvm:/dev/nvidia-uvm"
    runtime: nvidia
    privileged: false
    cap_add:
      - SYS_ADMIN
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9400/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "security.scan=enabled"
      - "security.profile=monitoring"
      - "security.component=dcgm-exporter"
    <<: *app-security

  # MLflow Model Registry - Model versioning and deployment
  mlflow-model-registry-rtx4090:
    build:
      context: .
      dockerfile: docker/services/Dockerfile.mlflow
      args:
        - MLFLOW_VERSION=2.8.1
    environment:
      # MLflow configuration
      - MLFLOW_BACKEND_STORE_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@mlflow-postgres:5432/mlflow
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow-artifacts
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
      - AWS_ACCESS_KEY_ID=${MINIO_ACCESS_KEY}
      - AWS_SECRET_ACCESS_KEY=${MINIO_SECRET_KEY}
      
      # Model serving
      - MLFLOW_DEPLOYMENTS_TARGET=torchserve
      - TORCHSERVE_MANAGEMENT_URL=http://torchserve-rtx4090:8081
      
      # GPU tracking
      - NVIDIA_VISIBLE_DEVICES=0
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "5001:5000"  # MLflow UI (offset to avoid conflict)
    volumes:
      - mlflow_artifacts:/opt/mlflow/artifacts
      - ./src/ml/models:/workspace/models:ro
    networks:
      - ai-network
      - backend-network
      - monitoring-network
    depends_on:
      - mlflow-postgres
      - minio
      - torchserve-rtx4090
    devices:
      - "/dev/nvidia0:/dev/nvidia0"
      - "/dev/nvidiactl:/dev/nvidiactl"
      - "/dev/nvidia-uvm:/dev/nvidia-uvm"
    runtime: nvidia
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:5000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.mlflow-models.rule=Host(`mlflow-models.gameforge.local`)"
      - "traefik.http.routers.mlflow-models.tls=true"
      - "traefik.http.services.mlflow-models.loadbalancer.server.port=5000"
      - "security.scan=enabled"
      - "security.profile=ml-platform"
      - "security.component=mlflow-models"
    <<: [*app-security, *gpu-inference-resources]

# ========================================================================
# Network Configuration (Isolated)
# ========================================================================
networks:
  frontend:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "false"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.bridge.host_binding_ipv4: "127.0.0.1"
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/24
          gateway: 172.20.0.1

  backend:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.21.0.0/24
          gateway: 172.21.0.1

  monitoring:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.22.0.0/24
          gateway: 172.22.0.1

  # Phase 4: Vault Network for Model Security
  vault-network:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.23.0.0/24
          gateway: 172.23.0.1

  # Phase 3: Security Pipeline Network
  security-pipeline:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.24.0.0/24
          gateway: 172.24.0.1

  # GPU Workload Network - High-performance GPU compute isolation
  gpu-network:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "false"
    ipam:
      driver: default
      config:
        - subnet: 172.25.0.0/24
          gateway: 172.25.0.1

  # ML Platform Network - MLflow and Model Registry
  ml-network:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
    ipam:
      driver: default
      config:
        - subnet: 172.26.0.0/24
          gateway: 172.26.0.1

  # AI Platform Network - Ray, KubeFlow, TorchServe for RTX 4090
  ai-network:
    driver: bridge
    internal: true
    driver_opts:
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "false"
    ipam:
      driver: default
      config:
        - subnet: 172.27.0.0/24
          gateway: 172.27.0.1

# ========================================================================
# Volume Configuration (Secure)
# ========================================================================
volumes:
  # Security infrastructure shared volume
  security-shared:
    driver: local
    
  gameforge-logs:
    driver: local

  gameforge-cache:
    driver: local

  gameforge-assets:
    driver: local

  gameforge-models:
    driver: local

  # Phase 4: Model Cache Volume (Temporary, secure)
  model-cache:
    driver: local

  # GPU Shared Memory Volume - High-performance IPC for GPU workloads
  gpu-shared-memory:
    driver: tmpfs
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: "size=8G,uid=1001,gid=1001,mode=1777"

  # Training Data Volume - Large dataset storage for model training
  training-data:
    driver: local
    driver_opts:
      type: none
      device: ${TRAINING_DATA_PATH:-./volumes/training-data}
      o: bind,rw

  # Model Checkpoints Volume - Training checkpoint persistence
  model-checkpoints:
    driver: local
    driver_opts:
      type: none
      device: ${CHECKPOINT_PATH:-./volumes/model-checkpoints}
      o: bind,rw

  # Phase 4: Monitoring Data Volume
  monitoring-data:
    driver: local

  # Phase 4: Vault Data Volume
  vault-data:
    driver: local

  # Phase 4: Vault Logs Volume
  vault-logs:
    driver: local

  postgres-data:
    driver: local

  postgres-logs:
    driver: local

  redis-data:
    driver: local

  elasticsearch-data:
    driver: local

  elasticsearch-logs:
    driver: local

  logstash-data:
    driver: local

  nginx-logs:
    driver: local

  static-files:
    driver: local

  backup-data:
    driver: local

  prometheus-data:
    driver: local

  grafana-data:
    driver: local

  # ========================================================================
  # Observability & Tracing Volumes
  # ========================================================================
  otel-collector-data:
    driver: local

  jaeger-data:
    driver: local

  # ========================================================================
  # Alerting & Notification Volumes
  # ========================================================================
  alertmanager-data:
    driver: local

  notification-templates:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/monitoring/notifications/templates
      o: bind,ro

  notification-logs:
    driver: local

  # ========================================================================
  # MLflow Platform Volumes
  # ========================================================================
  mlflow-postgres-data:
    driver: local

  mlflow-postgres-logs:
    driver: local

  mlflow-redis-data:
    driver: local

  mlflow-data:
    driver: local

  mlflow-logs:
    driver: local

  mlflow-registry-data:
    driver: local

  mlflow-registry-logs:
    driver: local

  mlflow-canary-data:
    driver: local

  mlflow-canary-logs:
    driver: local

  # Dataset versioning cache volume
  dataset_cache:
    driver: local

  # ========================================================================
  # Phase 3: Security Pipeline Volumes (Simplified for Windows deployment)
  # ========================================================================
  security-scanner-cache:
    driver: local
  security-scanner-reports:
    driver: local
  sbom-reports:
    driver: local
  sbom-cache:
    driver: local
  cosign-keys:
    driver: local
  signing-reports:
    driver: local
  policy-engine-data:
    driver: local
  policy-engine-logs:
    driver: local
  security-metrics-data:
    driver: local
  security-metrics-rules:
    driver: local
  security-dashboard-data:
    driver: local
  harbor-registry-data:
    driver: local
  harbor-registry-logs:
    driver: local
  harbor-registry-secret:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/harbor-secret
      o: bind,nodev,nosuid

  harbor-registry-ca:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/harbor-ca
      o: bind,nodev,nosuid,ro

  # Clair Scanner Volumes (Phase 3)
  clair-scanner-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/clair-data
      o: bind,nodev,nosuid

  clair-scanner-logs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/clair-logs
      o: bind,nodev,nosuid

  # Notary Server Volumes (Phase 3)
  notary-server-data:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/notary-data
      o: bind,nodev,nosuid

  notary-server-certs:
    driver: local
    driver_opts:
      type: none
      device: ${PWD}/volumes/security/notary-certs
      o: bind,nodev,nosuid,ro

  # ========================================================================
  # AI Platform Volumes - Ray, KubeFlow, TorchServe for RTX 4090
  # ========================================================================
  
  # TorchServe Model Storage
  torchserve_models:
    driver: local
    driver_opts:
      type: none
      device: ${TORCHSERVE_MODEL_PATH:-./volumes/ai/torchserve/models}
      o: bind,rw

  torchserve_logs:
    driver: local
    driver_opts:
      type: none
      device: ${TORCHSERVE_LOG_PATH:-./volumes/ai/torchserve/logs}
      o: bind,rw

  # Ray Cluster Storage
  ray_logs:
    driver: local
    driver_opts:
      type: none
      device: ${RAY_LOG_PATH:-./volumes/ai/ray/logs}
      o: bind,rw

  ray_data:
    driver: local
    driver_opts:
      type: none
      device: ${RAY_DATA_PATH:-./volumes/ai/ray/data}
      o: bind,rw

  # KubeFlow Pipeline Storage
  kubeflow_pipelines:
    driver: local
    driver_opts:
      type: none
      device: ${KUBEFLOW_PIPELINE_PATH:-./volumes/ai/kubeflow/pipelines}
      o: bind,rw

  kubeflow_metadata:
    driver: local
    driver_opts:
      type: none
      device: ${KUBEFLOW_METADATA_PATH:-./volumes/ai/kubeflow/metadata}
      o: bind,rw

  # MLflow Artifacts for AI Platform
  mlflow_artifacts:
    driver: local
    driver_opts:
      type: none
      device: ${MLFLOW_ARTIFACT_PATH:-./volumes/ai/mlflow/artifacts}
      o: bind,rw
