[{
  "name": "sdxl-optimized",
  "image": "public.ecr.aws/docker/library/python:3.11-slim",
  "essential": true,
  "portMappings": [{
    "containerPort": 8080,
    "protocol": "tcp",
    "name": "sdxl-port"
  }],
  "logConfiguration": {
    "logDriver": "awslogs",
    "options": {
      "awslogs-group": "/ecs/gameforge-sdxl-optimized",
      "awslogs-region": "us-east-1",
      "awslogs-stream-prefix": "ecs"
    }
  },
  "environment": [
    {"name": "PORT", "value": "8080"},
    {"name": "HF_HOME", "value": "/tmp/huggingface_cache"}
  ],
  "command": [
    "bash", "-c",
    "apt-get update && apt-get install -y curl git && pip install --no-cache-dir torch torchvision --index-url https://download.pytorch.org/whl/cpu && pip install --no-cache-dir fastapi uvicorn diffusers transformers accelerate safetensors psutil && python -c 'from fastapi import FastAPI, HTTPException; from pydantic import BaseModel; import torch, io, base64, os, uvicorn, logging, gc, asyncio; from diffusers import DiffusionPipeline; from contextlib import asynccontextmanager; logging.basicConfig(level=logging.INFO); logger = logging.getLogger(__name__); MODEL_CACHE = {}; MODEL_ID = \"segmind/SSD-1B\"; class ImageRequest(BaseModel): prompt: str; width: int = 512; height: int = 512; steps: int = 20; guidance_scale: float = 7.5; class ImageResponse(BaseModel): image: str; metadata: dict; async def load_model(): global MODEL_CACHE; if \"pipeline\" in MODEL_CACHE: logger.info(\"Model cached\"); return; logger.info(f\"Loading {MODEL_ID}...\"); pipeline = DiffusionPipeline.from_pretrained(MODEL_ID, torch_dtype=torch.float32, safety_checker=None, requires_safety_checker=False); pipeline = pipeline.to(\"cpu\"); pipeline.enable_attention_slicing(); MODEL_CACHE[\"pipeline\"] = pipeline; MODEL_CACHE[\"model_id\"] = MODEL_ID; logger.info(\"SDXL model loaded with caching\"); @asynccontextmanager; async def lifespan(app): logger.info(\"Starting GameForge SDXL Optimized...\"); await load_model(); yield; app = FastAPI(title=\"GameForge SDXL Optimized\", version=\"2.1.0\", lifespan=lifespan); @app.get(\"/health\"); async def health(): return {\"status\": \"healthy\" if \"pipeline\" in MODEL_CACHE else \"loading\", \"version\": \"2.1.0\", \"service\": \"sdxl-optimized\", \"models_loaded\": \"pipeline\" in MODEL_CACHE, \"model\": MODEL_CACHE.get(\"model_id\", \"loading\")}; @app.get(\"/model-status\"); async def model_status(): if \"pipeline\" not in MODEL_CACHE: raise HTTPException(status_code=503, detail=\"Loading\"); return {\"loaded\": True, \"model_id\": MODEL_CACHE[\"model_id\"], \"device\": \"cpu\", \"optimizations\": {\"attention_slicing\": True, \"model_caching\": True}}; @app.post(\"/generate\", response_model=ImageResponse); async def generate_image(request: ImageRequest): if \"pipeline\" not in MODEL_CACHE: raise HTTPException(status_code=503, detail=\"Model loading\"); pipeline = MODEL_CACHE[\"pipeline\"]; logger.info(f\"Generating: {request.prompt[:50]}...\"); with torch.inference_mode(): result = pipeline(prompt=request.prompt, width=request.width, height=request.height, num_inference_steps=request.steps, guidance_scale=request.guidance_scale, output_type=\"pil\"); image = result.images[0]; buffer = io.BytesIO(); image.save(buffer, format=\"PNG\"); img_base64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\"); gc.collect(); return ImageResponse(image=img_base64, metadata={\"prompt\": request.prompt, \"width\": request.width, \"height\": request.height, \"steps\": request.steps, \"model\": MODEL_CACHE[\"model_id\"], \"device\": \"cpu\", \"format\": \"PNG\", \"service\": \"sdxl-optimized\", \"cached\": True}); uvicorn.run(app, host=\"0.0.0.0\", port=int(os.getenv(\"PORT\", 8080)))'"
  ],
  "healthCheck": {
    "command": ["CMD-SHELL", "curl -f http://localhost:8080/health || exit 1"],
    "interval": 60,
    "timeout": 10,
    "retries": 3,
    "startPeriod": 300
  },
  "stopTimeout": 60
}]
